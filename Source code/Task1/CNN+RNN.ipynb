{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN+RNN",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8g5P0W-gqy2",
        "outputId": "9cab8baa-053e-4c81-9a35-e44d57aec01f"
      },
      "source": [
        "import json\n",
        "import string\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import pip\n",
        "from nltk import collections\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import svm, metrics\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import LSTM, GRU, TimeDistributed, Bidirectional\n",
        "from keras import backend as K\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import requests, zipfile, io\n",
        "zip_file_url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\"\n",
        "r = requests.get(zip_file_url)\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()\n",
        "\n",
        "zip_file_url2 = \"http://nlp.stanford.edu/data/glove.twitter.27B.zip\"\n",
        "r2 = requests.get(zip_file_url2)\n",
        "z2 = zipfile.ZipFile(io.BytesIO(r2.content))\n",
        "z2.extractall()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tlTlEzthnkP",
        "outputId": "f8266b57-3bdd-4937-ca54-edea218e87b6"
      },
      "source": [
        "def process_data(read_data_filepath, read_label_filepath):\n",
        "  label_map = {'non-rumour': 0, 'rumour': 1}\n",
        "  index_map = {v: k for k, v in label_map.items()}\n",
        "  with open(read_data_filepath, 'r') as f:\n",
        "    source_id = []\n",
        "    text = []\n",
        "    label = [] \n",
        "    for line in f:\n",
        "      line = json.loads(line)\n",
        "      source_id.append(line[0]['id_str'])\n",
        "      text.append(line[0]['text'])\n",
        "    df_data = pd.DataFrame({'id':source_id,'text':text, 'label':None})\n",
        "  if read_label_filepath is not None:\n",
        "    with open(read_label_filepath, 'r') as f:\n",
        "      id, label = [], []\n",
        "      temp = json.loads(f.read())\n",
        "      for key, val in temp.items():\n",
        "        id.append(key)\n",
        "        label.append(val)\n",
        "      # label_data = pd.DataFrame(list(zip(id, label)))\n",
        "      # label_data.columns = ['id', 'label']\n",
        "      for i in range(len(label)):\n",
        "          label[i] = 1 if label[i] == \"rumour\" else 0 #convert_label(label_data[\"label\"][i])\n",
        "          if source_id[i] == id[i]:\n",
        "            df_data.label[i] = label[i]\n",
        "    return df_data\n",
        "  else:\n",
        "    return df_data[['id', 'text']]\n",
        "\n",
        "df_dev = process_data('/content/drive/My Drive/data/dev.data.jsonl', '/content/drive/My Drive/data/dev.label.json')\n",
        "df_train = process_data('/content/drive/My Drive/data/train.data.jsonl', '/content/drive/My Drive/data/train.label.json')\n",
        "df_test = process_data('/content/drive/My Drive/data/test.data.jsonl', None)\n",
        "\n",
        "# a list of events, and each event is a list of tweets (source tweet + reactions)\n",
        "print(\"Number of train data =\", len(df_train[\"text\"]))\n",
        "print(\"Number of dev data =\", len(df_dev[\"text\"]))\n",
        "print(\"Number of test data =\", len(df_test[\"text\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of train data = 4641\n",
            "Number of dev data = 580\n",
            "Number of test data = 581\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKhbMh5Qhng2",
        "outputId": "da0309b1-f884-428c-e800-276d507410bb"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# clean data\n",
        "default_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "# will have to add the following custom\n",
        "custom_stopwords = {\"http://\", \"rt\", \"co\", \"https://\", \"www\", \"@\"}\n",
        "all_stopwords = default_stopwords | custom_stopwords\n",
        "eng_stemmer = nltk.stem.SnowballStemmer('english')\n",
        "tt = TweetTokenizer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def preprocess_data(df):\n",
        "    print(\"Started preprocessing!\")\n",
        "    text = df['text'].apply(str)\n",
        "    for i in range(len(text)):\n",
        "        text[i] = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+))', '', text[i])\n",
        "        text[i] = re.sub(r'@[^\\s]+', '', text[i])\n",
        "        text[i] = tt.tokenize(text[i])\n",
        "        # remove single character words\n",
        "        text[i] = [word for word in text[i] if len(word) > 1]\n",
        "        # convert to lower case\n",
        "        text[i] = [word.lower() for word in text[i]]\n",
        "        # removing numbers\n",
        "        text[i] = [word for word in text[i] if word.isalpha()]\n",
        "        # stem the words\n",
        "        text[i] = [lemmatizer.lemmatize(word) for word in text[i]]\n",
        "        # remove stopwords\n",
        "        text[i] = [word for word in text[i] if word not in default_stopwords]\n",
        "        text[i] = \" \".join(text[i])\n",
        "        df['text'][i] = text[i]\n",
        "    return df['text']\n",
        "\n",
        "\n",
        "train_texts = preprocess_data(df_train)\n",
        "dev_texts = preprocess_data(df_dev)\n",
        "test_texts = preprocess_data(df_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Started preprocessing!\n",
            "Started preprocessing!\n",
            "Started preprocessing!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LWY31ilipx09",
        "outputId": "e4699088-b236-4210-c5ac-4b531132affd"
      },
      "source": [
        "test_texts[9]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'germanwings plane crash southern france french prime minister say soon'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1LAXgJ-hnbh"
      },
      "source": [
        "## Get Train data and Validation data for model 1\n",
        "# x_train, x_test, y_train, y_test = train_test_split(train_texts, np.array(df_train[\"label\"], dtype=int), test_size=0.1)\n",
        "x_train = train_texts\n",
        "y_train = np.array(df_train[\"label\"].apply(int))\n",
        "x_dev = dev_texts\n",
        "y_dev = np.array(df_dev[\"label\"].apply(int))\n",
        "x_test = test_texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEO7HfK6hnYa"
      },
      "source": [
        "def convert_label_tofile(label):\n",
        "    if int(label) == 1:\n",
        "        label = \"rumour\"\n",
        "        return label\n",
        "    else:\n",
        "        label = \"non-rumour\"\n",
        "        return label\n",
        "\n",
        "\n",
        "def write_pred(id, labels, filename):\n",
        "    for i in range(len(labels)):\n",
        "        labels[i][0] = convert_label_tofile(labels[i][0])\n",
        "    dic = collections.OrderedDict()\n",
        "    for i in range(len(id)):\n",
        "        dic[id[i]] = labels[i][0]\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(dic, f)\n",
        "    print(\"save finished\")\n",
        "\n",
        "\n",
        "def convert_prob(y_pred):\n",
        "    for i in range(len(y_pred)):\n",
        "        if y_pred[i] > 0.5:\n",
        "            y_pred[i] = 1\n",
        "        else:\n",
        "            y_pred[i] = 0\n",
        "    return y_pred\n",
        "\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "HwoVQtz0oj08",
        "outputId": "f004e32c-93fa-445b-852c-fa550234e131"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "doc_len = x_train.apply(lambda words: len(words.split(\" \")))\n",
        "max_seq_len = np.round(doc_len.mean() + doc_len.std()).astype(int)\n",
        "\n",
        "sns.distplot(doc_len, hist=True, kde=True, color='b', label='doc len')\n",
        "plt.axvline(x=max_seq_len, color='k', linestyle='--', label='max len')\n",
        "plt.title('text length'); plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43R_uwlThnWU",
        "outputId": "0e22a63c-35a1-4767-fe14-47b4ae176b0d"
      },
      "source": [
        "MAX_NB_WORDS = 100000\n",
        "print(\"tokenizing input data...\")\n",
        "tokenizer = Tokenizer(oov_token=\"<UNK>\", num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "#leaky\n",
        "xseq_train = tokenizer.texts_to_sequences(x_train)\n",
        "xseq_dev = tokenizer.texts_to_sequences(x_dev)\n",
        "xseq_test = tokenizer.texts_to_sequences(x_test)\n",
        "word_index = tokenizer.word_index\n",
        "print(\"dictionary size: \", len(word_index))\n",
        "\n",
        "# vocab_size = 46660\n",
        "#pad sequences\n",
        "xseq_train = pad_sequences(xseq_train, padding='post', maxlen=max_seq_len)\n",
        "xseq_dev = pad_sequences(xseq_dev, padding='post', maxlen=max_seq_len)\n",
        "xseq_test = pad_sequences(xseq_test, padding='post', maxlen=max_seq_len)\n",
        "\n",
        "# embedding_dim = 10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenizing input data...\n",
            "dictionary size:  5085\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFlR49P9hnTd"
      },
      "source": [
        "# \"\"\"\n",
        "# LSTM\n",
        "# \"\"\"\n",
        "# # word order preserved with this architecture\n",
        "# model3 = Sequential(name=\"lstm\")\n",
        "# model3.add(layers.Embedding(input_dim=vocab_size,\n",
        "#                             output_dim=embedding_dim,\n",
        "#                             input_length=maxlen))\n",
        "# model3.add(LSTM(10))\n",
        "# model3.add(layers.Dense(1, activation='sigmoid'))\n",
        "# model3.compile(optimizer='adam',\n",
        "#                loss='binary_crossentropy',\n",
        "#                metrics=['acc', f1_m, precision_m, recall_m])\n",
        "# model3.summary()\n",
        "\n",
        "# # training\n",
        "# model3.fit(xseq_train, y_train, epochs=8, verbose=True, validation_data=(xseq_dev, y_dev), batch_size=10)\n",
        "# loss, accuracy, f1_score, precision, recall = model3.evaluate(xseq_dev, y_dev, verbose=False)\n",
        "# print(\"\\nTesting LSTM f1_score:  {:.4f}\".format(f1_score))\n",
        "\n",
        "# # predict test set\n",
        "# y_pred_test3 = model3.predict(xseq_test)\n",
        "# y_pred_test3 = convert_prob(y_pred_test3)\n",
        "# y_pred_test3 = np.array(y_pred_test3).tolist()\n",
        "# write_pred(df_test[\"id\"], y_pred_test3, \"test-output.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDuiQbvThnRS"
      },
      "source": [
        "# For Training\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from keras import backend as K\n",
        "from keras import regularizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from keras.utils import plot_model\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78dymXOYmffu"
      },
      "source": [
        "#Shape and Train with LSTM\n",
        "#training params\n",
        "batch_size = 256 \n",
        "num_epochs = 40\n",
        "\n",
        "#model parameters\n",
        "num_filters = 64 \n",
        "embed_dim = 300 \n",
        "weight_decay = 1e-4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljbBqWv6q6DU",
        "outputId": "0bd4ab6c-cab2-46a7-fbd4-d386dee81ffe"
      },
      "source": [
        "#embedding matrix\n",
        "import os, re, csv, math, codecs\n",
        "\n",
        "print('preparing embedding matrix...')\n",
        "\n",
        "words_not_found = []\n",
        "nb_words = min(MAX_NB_WORDS, len(word_index)+1)\n",
        "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
        "\n",
        "print('loading word embeddings...')\n",
        "\n",
        "embeddings_index = {}\n",
        "f = codecs.open('wiki-news-300d-1M.vec', encoding='utf-8')\n",
        "\n",
        "from tqdm import tqdm\n",
        "for line in tqdm(f):\n",
        "    values = line.rstrip().rsplit(' ')\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('found %s word vectors' % len(embeddings_index))\n",
        "for word, i in word_index.items():\n",
        "    if i >= nb_words:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        words_not_found.append(word)\n",
        "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "999it [00:00, 9987.63it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "preparing embedding matrix...\n",
            "loading word embeddings...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "999995it [01:47, 9294.55it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "found 999995 word vectors\n",
            "number of null word embeddings: 227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLdpfJyjuROV",
        "outputId": "4d20fa21-f568-4335-8321-d38e6be9adbd"
      },
      "source": [
        "print(\"sample words not found: \", np.random.choice(words_not_found, 10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample words not found:  ['brinsolaro' 'haron' 'barcroft' 'monis' 'banjarnegara' 'gunm' 'kouachi'\n",
            " 'phorphet' 'norad' 'florissant']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY7T9_rCmIMQ",
        "outputId": "8aa8a43e-3e83-4bde-949e-b99a9a2c566d"
      },
      "source": [
        "from keras.layers import BatchNormalization\n",
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(Embedding(nb_words, embed_dim, input_length=max_seq_len, weights=[embedding_matrix], trainable=False))\n",
        "\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Bidirectional(LSTM(64,return_sequences= True)))\n",
        "model.add(Bidirectional(LSTM(64,return_sequences= True)))\n",
        "model.add(Bidirectional(LSTM(64,return_sequences= True)))\n",
        "model.add(Bidirectional(LSTM(64,return_sequences= True)))\n",
        "model.add(Bidirectional(LSTM(32)))\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 11, 300)           1525800   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 11, 300)           0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 11, 32)            9632      \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 11, 128)           49664     \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 11, 128)           98816     \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 11, 128)           98816     \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 11, 128)           98816     \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 64)                41216     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,924,873\n",
            "Trainable params: 399,073\n",
            "Non-trainable params: 1,525,800\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWwQSR5PmIJD"
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', f1_m, precision_m, recall_m])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nf8FdL28mIGw",
        "outputId": "6c7fa2a9-85ea-440f-8f93-6ccd98067095"
      },
      "source": [
        "# training\n",
        "model.fit(xseq_train, y_train, epochs=num_epochs, verbose=True, validation_data=(xseq_dev, y_dev), batch_size=256)\n",
        "loss, accuracy, f1_score, precision, recall = model.evaluate(xseq_dev, y_dev, verbose=False)\n",
        "print(\"\\nTesting word sequence f1_score:  {:.4f}\".format(f1_score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "19/19 [==============================] - 27s 568ms/step - loss: 0.6671 - accuracy: 0.6429 - f1_m: 0.0257 - precision_m: 0.0579 - recall_m: 0.0165 - val_loss: 0.6014 - val_accuracy: 0.6776 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 2/40\n",
            "19/19 [==============================] - 6s 322ms/step - loss: 0.5529 - accuracy: 0.7087 - f1_m: 0.3303 - precision_m: 0.4919 - recall_m: 0.3240 - val_loss: 0.4386 - val_accuracy: 0.7828 - val_f1_m: 0.6627 - val_precision_m: 0.6020 - val_recall_m: 0.7406\n",
            "Epoch 3/40\n",
            "19/19 [==============================] - 6s 318ms/step - loss: 0.4547 - accuracy: 0.7878 - f1_m: 0.6982 - precision_m: 0.6863 - recall_m: 0.7143 - val_loss: 0.4159 - val_accuracy: 0.7948 - val_f1_m: 0.6834 - val_precision_m: 0.6169 - val_recall_m: 0.7684\n",
            "Epoch 4/40\n",
            "19/19 [==============================] - 6s 319ms/step - loss: 0.4476 - accuracy: 0.7936 - f1_m: 0.7105 - precision_m: 0.6925 - recall_m: 0.7445 - val_loss: 0.4441 - val_accuracy: 0.7793 - val_f1_m: 0.6914 - val_precision_m: 0.5828 - val_recall_m: 0.8524\n",
            "Epoch 5/40\n",
            "19/19 [==============================] - 6s 329ms/step - loss: 0.4619 - accuracy: 0.7936 - f1_m: 0.7140 - precision_m: 0.6964 - recall_m: 0.7695 - val_loss: 0.3776 - val_accuracy: 0.8207 - val_f1_m: 0.6909 - val_precision_m: 0.6894 - val_recall_m: 0.6980\n",
            "Epoch 6/40\n",
            "19/19 [==============================] - 6s 319ms/step - loss: 0.4116 - accuracy: 0.8134 - f1_m: 0.7221 - precision_m: 0.7289 - recall_m: 0.7223 - val_loss: 0.3793 - val_accuracy: 0.8069 - val_f1_m: 0.6982 - val_precision_m: 0.6366 - val_recall_m: 0.7783\n",
            "Epoch 7/40\n",
            "19/19 [==============================] - 6s 321ms/step - loss: 0.4176 - accuracy: 0.8117 - f1_m: 0.7210 - precision_m: 0.6978 - recall_m: 0.7523 - val_loss: 0.3708 - val_accuracy: 0.8224 - val_f1_m: 0.6974 - val_precision_m: 0.6832 - val_recall_m: 0.7171\n",
            "Epoch 8/40\n",
            "19/19 [==============================] - 6s 315ms/step - loss: 0.4029 - accuracy: 0.8250 - f1_m: 0.7272 - precision_m: 0.7571 - recall_m: 0.7134 - val_loss: 0.3640 - val_accuracy: 0.8207 - val_f1_m: 0.7058 - val_precision_m: 0.6724 - val_recall_m: 0.7499\n",
            "Epoch 9/40\n",
            "19/19 [==============================] - 6s 317ms/step - loss: 0.3985 - accuracy: 0.8273 - f1_m: 0.7476 - precision_m: 0.7366 - recall_m: 0.7680 - val_loss: 0.3570 - val_accuracy: 0.8259 - val_f1_m: 0.6996 - val_precision_m: 0.6943 - val_recall_m: 0.7141\n",
            "Epoch 10/40\n",
            "19/19 [==============================] - 6s 318ms/step - loss: 0.3758 - accuracy: 0.8401 - f1_m: 0.7703 - precision_m: 0.7557 - recall_m: 0.7878 - val_loss: 0.3553 - val_accuracy: 0.8276 - val_f1_m: 0.7148 - val_precision_m: 0.6750 - val_recall_m: 0.7641\n",
            "Epoch 11/40\n",
            "19/19 [==============================] - 6s 317ms/step - loss: 0.3708 - accuracy: 0.8500 - f1_m: 0.7855 - precision_m: 0.7774 - recall_m: 0.7960 - val_loss: 0.3544 - val_accuracy: 0.8328 - val_f1_m: 0.7069 - val_precision_m: 0.7147 - val_recall_m: 0.7067\n",
            "Epoch 12/40\n",
            "19/19 [==============================] - 6s 319ms/step - loss: 0.3813 - accuracy: 0.8388 - f1_m: 0.7629 - precision_m: 0.7671 - recall_m: 0.7617 - val_loss: 0.3448 - val_accuracy: 0.8379 - val_f1_m: 0.7386 - val_precision_m: 0.7083 - val_recall_m: 0.7777\n",
            "Epoch 13/40\n",
            "19/19 [==============================] - 6s 318ms/step - loss: 0.3583 - accuracy: 0.8484 - f1_m: 0.7843 - precision_m: 0.7715 - recall_m: 0.8028 - val_loss: 0.3417 - val_accuracy: 0.8414 - val_f1_m: 0.7284 - val_precision_m: 0.7206 - val_recall_m: 0.7418\n",
            "Epoch 14/40\n",
            "19/19 [==============================] - 6s 320ms/step - loss: 0.3731 - accuracy: 0.8432 - f1_m: 0.7715 - precision_m: 0.7774 - recall_m: 0.7756 - val_loss: 0.3337 - val_accuracy: 0.8569 - val_f1_m: 0.7412 - val_precision_m: 0.7558 - val_recall_m: 0.7344\n",
            "Epoch 15/40\n",
            "19/19 [==============================] - 6s 320ms/step - loss: 0.3581 - accuracy: 0.8391 - f1_m: 0.7582 - precision_m: 0.7695 - recall_m: 0.7500 - val_loss: 0.3322 - val_accuracy: 0.8552 - val_f1_m: 0.7421 - val_precision_m: 0.7444 - val_recall_m: 0.7462\n",
            "Epoch 16/40\n",
            "19/19 [==============================] - 6s 318ms/step - loss: 0.3581 - accuracy: 0.8432 - f1_m: 0.7595 - precision_m: 0.7716 - recall_m: 0.7514 - val_loss: 0.3408 - val_accuracy: 0.8466 - val_f1_m: 0.7555 - val_precision_m: 0.7257 - val_recall_m: 0.7986\n",
            "Epoch 17/40\n",
            "19/19 [==============================] - 6s 319ms/step - loss: 0.3403 - accuracy: 0.8565 - f1_m: 0.7850 - precision_m: 0.8037 - recall_m: 0.7743 - val_loss: 0.3377 - val_accuracy: 0.8552 - val_f1_m: 0.7470 - val_precision_m: 0.7768 - val_recall_m: 0.7320\n",
            "Epoch 18/40\n",
            "19/19 [==============================] - 6s 319ms/step - loss: 0.3527 - accuracy: 0.8484 - f1_m: 0.7658 - precision_m: 0.8163 - recall_m: 0.7342 - val_loss: 0.3389 - val_accuracy: 0.8466 - val_f1_m: 0.7628 - val_precision_m: 0.7217 - val_recall_m: 0.8147\n",
            "Epoch 19/40\n",
            "19/19 [==============================] - 6s 320ms/step - loss: 0.3597 - accuracy: 0.8497 - f1_m: 0.7853 - precision_m: 0.7789 - recall_m: 0.7986 - val_loss: 0.3485 - val_accuracy: 0.8310 - val_f1_m: 0.7462 - val_precision_m: 0.6929 - val_recall_m: 0.8159\n",
            "Epoch 20/40\n",
            "19/19 [==============================] - 6s 318ms/step - loss: 0.3279 - accuracy: 0.8598 - f1_m: 0.7849 - precision_m: 0.8229 - recall_m: 0.7546 - val_loss: 0.3371 - val_accuracy: 0.8293 - val_f1_m: 0.7435 - val_precision_m: 0.6914 - val_recall_m: 0.8110\n",
            "Epoch 21/40\n",
            "19/19 [==============================] - 6s 320ms/step - loss: 0.3286 - accuracy: 0.8617 - f1_m: 0.8047 - precision_m: 0.7763 - recall_m: 0.8390 - val_loss: 0.3268 - val_accuracy: 0.8362 - val_f1_m: 0.7350 - val_precision_m: 0.7294 - val_recall_m: 0.7530\n",
            "Epoch 22/40\n",
            "19/19 [==============================] - 6s 320ms/step - loss: 0.3106 - accuracy: 0.8674 - f1_m: 0.8078 - precision_m: 0.8084 - recall_m: 0.8098 - val_loss: 0.3251 - val_accuracy: 0.8414 - val_f1_m: 0.7408 - val_precision_m: 0.7361 - val_recall_m: 0.7567\n",
            "Epoch 23/40\n",
            "19/19 [==============================] - 6s 319ms/step - loss: 0.3026 - accuracy: 0.8707 - f1_m: 0.8073 - precision_m: 0.8136 - recall_m: 0.8065 - val_loss: 0.3271 - val_accuracy: 0.8431 - val_f1_m: 0.7614 - val_precision_m: 0.7117 - val_recall_m: 0.8221\n",
            "Epoch 24/40\n",
            "19/19 [==============================] - 6s 321ms/step - loss: 0.3129 - accuracy: 0.8630 - f1_m: 0.7985 - precision_m: 0.7983 - recall_m: 0.8000 - val_loss: 0.3231 - val_accuracy: 0.8448 - val_f1_m: 0.7642 - val_precision_m: 0.7107 - val_recall_m: 0.8295\n",
            "Epoch 25/40\n",
            "19/19 [==============================] - 6s 323ms/step - loss: 0.3060 - accuracy: 0.8698 - f1_m: 0.8059 - precision_m: 0.8038 - recall_m: 0.8124 - val_loss: 0.3587 - val_accuracy: 0.8310 - val_f1_m: 0.7742 - val_precision_m: 0.6723 - val_recall_m: 0.9147\n",
            "Epoch 26/40\n",
            "19/19 [==============================] - 6s 319ms/step - loss: 0.3155 - accuracy: 0.8651 - f1_m: 0.8104 - precision_m: 0.7828 - recall_m: 0.8499 - val_loss: 0.3482 - val_accuracy: 0.8517 - val_f1_m: 0.7835 - val_precision_m: 0.7112 - val_recall_m: 0.8746\n",
            "Epoch 27/40\n",
            "19/19 [==============================] - 6s 317ms/step - loss: 0.2970 - accuracy: 0.8798 - f1_m: 0.8268 - precision_m: 0.8029 - recall_m: 0.8548 - val_loss: 0.3385 - val_accuracy: 0.8397 - val_f1_m: 0.7732 - val_precision_m: 0.6963 - val_recall_m: 0.8752\n",
            "Epoch 28/40\n",
            "19/19 [==============================] - 6s 324ms/step - loss: 0.2940 - accuracy: 0.8803 - f1_m: 0.8264 - precision_m: 0.8233 - recall_m: 0.8315 - val_loss: 0.3209 - val_accuracy: 0.8552 - val_f1_m: 0.7625 - val_precision_m: 0.7950 - val_recall_m: 0.7449\n",
            "Epoch 29/40\n",
            "19/19 [==============================] - 6s 321ms/step - loss: 0.2979 - accuracy: 0.8760 - f1_m: 0.8084 - precision_m: 0.8593 - recall_m: 0.7689 - val_loss: 0.3324 - val_accuracy: 0.8431 - val_f1_m: 0.7571 - val_precision_m: 0.7380 - val_recall_m: 0.7844\n",
            "Epoch 30/40\n",
            "19/19 [==============================] - 6s 322ms/step - loss: 0.2870 - accuracy: 0.8748 - f1_m: 0.8106 - precision_m: 0.8513 - recall_m: 0.7777 - val_loss: 0.3437 - val_accuracy: 0.8552 - val_f1_m: 0.7716 - val_precision_m: 0.7453 - val_recall_m: 0.8024\n",
            "Epoch 31/40\n",
            "19/19 [==============================] - 6s 321ms/step - loss: 0.2641 - accuracy: 0.8927 - f1_m: 0.8402 - precision_m: 0.8514 - recall_m: 0.8324 - val_loss: 0.3331 - val_accuracy: 0.8431 - val_f1_m: 0.7625 - val_precision_m: 0.7258 - val_recall_m: 0.8079\n",
            "Epoch 32/40\n",
            "19/19 [==============================] - 6s 322ms/step - loss: 0.2785 - accuracy: 0.8841 - f1_m: 0.8370 - precision_m: 0.8242 - recall_m: 0.8520 - val_loss: 0.3229 - val_accuracy: 0.8534 - val_f1_m: 0.7681 - val_precision_m: 0.7544 - val_recall_m: 0.7931\n",
            "Epoch 33/40\n",
            "19/19 [==============================] - 6s 321ms/step - loss: 0.2733 - accuracy: 0.8827 - f1_m: 0.8191 - precision_m: 0.8486 - recall_m: 0.7949 - val_loss: 0.3261 - val_accuracy: 0.8483 - val_f1_m: 0.7761 - val_precision_m: 0.7316 - val_recall_m: 0.8320\n",
            "Epoch 34/40\n",
            "19/19 [==============================] - 6s 323ms/step - loss: 0.2917 - accuracy: 0.8737 - f1_m: 0.8125 - precision_m: 0.8249 - recall_m: 0.8100 - val_loss: 0.3218 - val_accuracy: 0.8500 - val_f1_m: 0.7715 - val_precision_m: 0.7297 - val_recall_m: 0.8227\n",
            "Epoch 35/40\n",
            "19/19 [==============================] - 6s 324ms/step - loss: 0.2716 - accuracy: 0.8773 - f1_m: 0.8210 - precision_m: 0.8297 - recall_m: 0.8168 - val_loss: 0.3263 - val_accuracy: 0.8569 - val_f1_m: 0.7743 - val_precision_m: 0.7491 - val_recall_m: 0.8067\n",
            "Epoch 36/40\n",
            "19/19 [==============================] - 6s 319ms/step - loss: 0.2468 - accuracy: 0.8921 - f1_m: 0.8376 - precision_m: 0.8407 - recall_m: 0.8380 - val_loss: 0.3267 - val_accuracy: 0.8603 - val_f1_m: 0.7688 - val_precision_m: 0.7583 - val_recall_m: 0.7863\n",
            "Epoch 37/40\n",
            "19/19 [==============================] - 6s 321ms/step - loss: 0.2789 - accuracy: 0.8850 - f1_m: 0.8312 - precision_m: 0.8453 - recall_m: 0.8224 - val_loss: 0.3590 - val_accuracy: 0.8362 - val_f1_m: 0.7689 - val_precision_m: 0.6925 - val_recall_m: 0.8666\n",
            "Epoch 38/40\n",
            "19/19 [==============================] - 8s 409ms/step - loss: 0.2592 - accuracy: 0.8870 - f1_m: 0.8288 - precision_m: 0.8323 - recall_m: 0.8312 - val_loss: 0.3226 - val_accuracy: 0.8569 - val_f1_m: 0.7873 - val_precision_m: 0.7545 - val_recall_m: 0.8283\n",
            "Epoch 39/40\n",
            "19/19 [==============================] - 6s 324ms/step - loss: 0.2414 - accuracy: 0.8955 - f1_m: 0.8449 - precision_m: 0.8407 - recall_m: 0.8504 - val_loss: 0.3279 - val_accuracy: 0.8500 - val_f1_m: 0.7785 - val_precision_m: 0.7322 - val_recall_m: 0.8357\n",
            "Epoch 40/40\n",
            "19/19 [==============================] - 6s 327ms/step - loss: 0.2518 - accuracy: 0.8963 - f1_m: 0.8518 - precision_m: 0.8557 - recall_m: 0.8498 - val_loss: 0.3306 - val_accuracy: 0.8431 - val_f1_m: 0.7717 - val_precision_m: 0.7231 - val_recall_m: 0.8320\n",
            "\n",
            "Testing word sequence f1_score:  0.7283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMchgWKkmID-",
        "outputId": "a49c57bd-79a8-4072-c77c-88104867ea7f"
      },
      "source": [
        "y_pred_test = model.predict_classes(xseq_test)\n",
        "y_pred_test = convert_prob(y_pred_test)\n",
        "y_pred_test = np.array(y_pred_test).tolist()\n",
        "write_pred(df_test[\"id\"], y_pred_test, \"test-cnn-0,7423.json\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "save finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV9aQOlSmIBl"
      },
      "source": [
        "\"\"\"\n",
        "GRU using wiki\n",
        "\"\"\"\n",
        "# we don't want the model to overwrite, dont we?\n",
        "keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL2IbSismH_A",
        "outputId": "7d2bfd9e-83be-469d-b639-f9f40db0c992"
      },
      "source": [
        "from keras.layers import BatchNormalization\n",
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(Embedding(nb_words,embed_dim,input_length=max_seq_len, weights=[embedding_matrix],trainable=False))\n",
        "model.add(Bidirectional(GRU(256, dropout=0.2, recurrent_dropout=0.1, return_sequences=True)))\n",
        "model.add(Bidirectional(GRU(256, dropout=0.2, recurrent_dropout=0.1)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 11, 300)           1525800   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 11, 512)           857088    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 512)               1182720   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 3,566,121\n",
            "Trainable params: 2,040,321\n",
            "Non-trainable params: 1,525,800\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpE1oXB8mH8u"
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', f1_m, precision_m, recall_m])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmGlNyuGmH5n",
        "outputId": "29408de4-06e2-4838-af22-5826fa7f807a"
      },
      "source": [
        "# training\n",
        "model.fit(xseq_train, y_train, epochs=num_epochs, verbose=True, validation_data=(xseq_dev, y_dev), batch_size=256)\n",
        "loss, accuracy, f1_score, precision, recall = model.evaluate(xseq_dev, y_dev, verbose=False)\n",
        "print(\"\\nTesting word sequence f1_score:  {:.4f}\".format(f1_score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "19/19 [==============================] - 39s 1s/step - loss: 0.6053 - accuracy: 0.6404 - f1_m: 0.2993 - precision_m: 0.3760 - recall_m: 0.3434 - val_loss: 0.3990 - val_accuracy: 0.8241 - val_f1_m: 0.6561 - val_precision_m: 0.7143 - val_recall_m: 0.6115\n",
            "Epoch 2/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.4176 - accuracy: 0.8028 - f1_m: 0.6874 - precision_m: 0.7456 - recall_m: 0.6483 - val_loss: 0.3773 - val_accuracy: 0.8259 - val_f1_m: 0.6943 - val_precision_m: 0.7179 - val_recall_m: 0.6819\n",
            "Epoch 3/40\n",
            "19/19 [==============================] - 25s 1s/step - loss: 0.3887 - accuracy: 0.8240 - f1_m: 0.7333 - precision_m: 0.7576 - recall_m: 0.7140 - val_loss: 0.3679 - val_accuracy: 0.8345 - val_f1_m: 0.6963 - val_precision_m: 0.7474 - val_recall_m: 0.6615\n",
            "Epoch 4/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.3782 - accuracy: 0.8352 - f1_m: 0.7572 - precision_m: 0.7719 - recall_m: 0.7557 - val_loss: 0.3714 - val_accuracy: 0.8310 - val_f1_m: 0.7143 - val_precision_m: 0.7053 - val_recall_m: 0.7351\n",
            "Epoch 5/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.3521 - accuracy: 0.8477 - f1_m: 0.7635 - precision_m: 0.7835 - recall_m: 0.7494 - val_loss: 0.3679 - val_accuracy: 0.8397 - val_f1_m: 0.7134 - val_precision_m: 0.7416 - val_recall_m: 0.6937\n",
            "Epoch 6/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.3334 - accuracy: 0.8575 - f1_m: 0.7803 - precision_m: 0.8128 - recall_m: 0.7538 - val_loss: 0.3619 - val_accuracy: 0.8397 - val_f1_m: 0.7281 - val_precision_m: 0.7688 - val_recall_m: 0.6998\n",
            "Epoch 7/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.3304 - accuracy: 0.8534 - f1_m: 0.7747 - precision_m: 0.8017 - recall_m: 0.7512 - val_loss: 0.3768 - val_accuracy: 0.8207 - val_f1_m: 0.6924 - val_precision_m: 0.6968 - val_recall_m: 0.6918\n",
            "Epoch 8/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.3110 - accuracy: 0.8622 - f1_m: 0.7942 - precision_m: 0.8016 - recall_m: 0.7921 - val_loss: 0.3909 - val_accuracy: 0.8362 - val_f1_m: 0.6864 - val_precision_m: 0.8709 - val_recall_m: 0.5695\n",
            "Epoch 9/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.3131 - accuracy: 0.8625 - f1_m: 0.7855 - precision_m: 0.8389 - recall_m: 0.7501 - val_loss: 0.3639 - val_accuracy: 0.8328 - val_f1_m: 0.7246 - val_precision_m: 0.6904 - val_recall_m: 0.7684\n",
            "Epoch 10/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.2986 - accuracy: 0.8665 - f1_m: 0.8020 - precision_m: 0.7999 - recall_m: 0.8161 - val_loss: 0.3657 - val_accuracy: 0.8310 - val_f1_m: 0.7522 - val_precision_m: 0.6838 - val_recall_m: 0.8431\n",
            "Epoch 11/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.2815 - accuracy: 0.8768 - f1_m: 0.8164 - precision_m: 0.8278 - recall_m: 0.8172 - val_loss: 0.3679 - val_accuracy: 0.8362 - val_f1_m: 0.7408 - val_precision_m: 0.7153 - val_recall_m: 0.7752\n",
            "Epoch 12/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.2701 - accuracy: 0.8806 - f1_m: 0.8296 - precision_m: 0.8238 - recall_m: 0.8388 - val_loss: 0.3620 - val_accuracy: 0.8310 - val_f1_m: 0.7278 - val_precision_m: 0.7196 - val_recall_m: 0.7424\n",
            "Epoch 13/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.2508 - accuracy: 0.8993 - f1_m: 0.8498 - precision_m: 0.8444 - recall_m: 0.8572 - val_loss: 0.3750 - val_accuracy: 0.8362 - val_f1_m: 0.7167 - val_precision_m: 0.7498 - val_recall_m: 0.6887\n",
            "Epoch 14/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.2416 - accuracy: 0.8940 - f1_m: 0.8378 - precision_m: 0.8550 - recall_m: 0.8289 - val_loss: 0.3879 - val_accuracy: 0.8414 - val_f1_m: 0.7073 - val_precision_m: 0.8126 - val_recall_m: 0.6282\n",
            "Epoch 15/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.2444 - accuracy: 0.8938 - f1_m: 0.8420 - precision_m: 0.8705 - recall_m: 0.8218 - val_loss: 0.4504 - val_accuracy: 0.8276 - val_f1_m: 0.6835 - val_precision_m: 0.8068 - val_recall_m: 0.5961\n",
            "Epoch 16/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.2236 - accuracy: 0.8994 - f1_m: 0.8489 - precision_m: 0.8686 - recall_m: 0.8367 - val_loss: 0.3927 - val_accuracy: 0.8293 - val_f1_m: 0.7160 - val_precision_m: 0.7452 - val_recall_m: 0.6918\n",
            "Epoch 17/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.1979 - accuracy: 0.9162 - f1_m: 0.8703 - precision_m: 0.8785 - recall_m: 0.8639 - val_loss: 0.4171 - val_accuracy: 0.8000 - val_f1_m: 0.6961 - val_precision_m: 0.6609 - val_recall_m: 0.7363\n",
            "Epoch 18/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.2096 - accuracy: 0.9087 - f1_m: 0.8705 - precision_m: 0.8549 - recall_m: 0.8897 - val_loss: 0.4117 - val_accuracy: 0.8138 - val_f1_m: 0.7233 - val_precision_m: 0.6774 - val_recall_m: 0.7819\n",
            "Epoch 19/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.1749 - accuracy: 0.9269 - f1_m: 0.8937 - precision_m: 0.8754 - recall_m: 0.9167 - val_loss: 0.4323 - val_accuracy: 0.8379 - val_f1_m: 0.7500 - val_precision_m: 0.7658 - val_recall_m: 0.7387\n",
            "Epoch 20/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.1583 - accuracy: 0.9297 - f1_m: 0.8969 - precision_m: 0.9024 - recall_m: 0.8953 - val_loss: 0.4292 - val_accuracy: 0.8276 - val_f1_m: 0.7472 - val_precision_m: 0.7146 - val_recall_m: 0.7893\n",
            "Epoch 21/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.1619 - accuracy: 0.9309 - f1_m: 0.8966 - precision_m: 0.8994 - recall_m: 0.8969 - val_loss: 0.4256 - val_accuracy: 0.8414 - val_f1_m: 0.7554 - val_precision_m: 0.7470 - val_recall_m: 0.7659\n",
            "Epoch 22/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.1549 - accuracy: 0.9379 - f1_m: 0.9071 - precision_m: 0.9108 - recall_m: 0.9041 - val_loss: 0.4969 - val_accuracy: 0.8155 - val_f1_m: 0.7211 - val_precision_m: 0.7046 - val_recall_m: 0.7387\n",
            "Epoch 23/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.1661 - accuracy: 0.9299 - f1_m: 0.8972 - precision_m: 0.8911 - recall_m: 0.9053 - val_loss: 0.4825 - val_accuracy: 0.8190 - val_f1_m: 0.7475 - val_precision_m: 0.6935 - val_recall_m: 0.8153\n",
            "Epoch 24/40\n",
            "19/19 [==============================] - 27s 1s/step - loss: 0.1325 - accuracy: 0.9476 - f1_m: 0.9258 - precision_m: 0.9160 - recall_m: 0.9395 - val_loss: 0.5002 - val_accuracy: 0.8397 - val_f1_m: 0.7595 - val_precision_m: 0.7707 - val_recall_m: 0.7616\n",
            "Epoch 25/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.1237 - accuracy: 0.9495 - f1_m: 0.9259 - precision_m: 0.9373 - recall_m: 0.9163 - val_loss: 0.5551 - val_accuracy: 0.8345 - val_f1_m: 0.7240 - val_precision_m: 0.8087 - val_recall_m: 0.6559\n",
            "Epoch 26/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.1282 - accuracy: 0.9462 - f1_m: 0.9205 - precision_m: 0.9355 - recall_m: 0.9082 - val_loss: 0.4996 - val_accuracy: 0.8345 - val_f1_m: 0.7411 - val_precision_m: 0.7712 - val_recall_m: 0.7183\n",
            "Epoch 27/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.1073 - accuracy: 0.9545 - f1_m: 0.9333 - precision_m: 0.9401 - recall_m: 0.9280 - val_loss: 0.5398 - val_accuracy: 0.8310 - val_f1_m: 0.7190 - val_precision_m: 0.7836 - val_recall_m: 0.6689\n",
            "Epoch 28/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.1131 - accuracy: 0.9553 - f1_m: 0.9319 - precision_m: 0.9425 - recall_m: 0.9230 - val_loss: 0.5252 - val_accuracy: 0.8534 - val_f1_m: 0.7687 - val_precision_m: 0.7514 - val_recall_m: 0.7956\n",
            "Epoch 29/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.1085 - accuracy: 0.9567 - f1_m: 0.9350 - precision_m: 0.9344 - recall_m: 0.9369 - val_loss: 0.5365 - val_accuracy: 0.8362 - val_f1_m: 0.7578 - val_precision_m: 0.7416 - val_recall_m: 0.7782\n",
            "Epoch 30/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.0952 - accuracy: 0.9661 - f1_m: 0.9507 - precision_m: 0.9538 - recall_m: 0.9486 - val_loss: 0.5294 - val_accuracy: 0.8483 - val_f1_m: 0.7655 - val_precision_m: 0.7954 - val_recall_m: 0.7480\n",
            "Epoch 31/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.0887 - accuracy: 0.9651 - f1_m: 0.9484 - precision_m: 0.9569 - recall_m: 0.9410 - val_loss: 0.5486 - val_accuracy: 0.8379 - val_f1_m: 0.7535 - val_precision_m: 0.7260 - val_recall_m: 0.7900\n",
            "Epoch 32/40\n",
            "19/19 [==============================] - 26s 1s/step - loss: 0.0863 - accuracy: 0.9644 - f1_m: 0.9472 - precision_m: 0.9518 - recall_m: 0.9440 - val_loss: 0.5852 - val_accuracy: 0.8448 - val_f1_m: 0.7549 - val_precision_m: 0.7822 - val_recall_m: 0.7387\n",
            "Epoch 33/40\n",
            "19/19 [==============================] - 25s 1s/step - loss: 0.0927 - accuracy: 0.9643 - f1_m: 0.9472 - precision_m: 0.9572 - recall_m: 0.9382 - val_loss: 0.5491 - val_accuracy: 0.8414 - val_f1_m: 0.7488 - val_precision_m: 0.8022 - val_recall_m: 0.7066\n",
            "Epoch 34/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.0788 - accuracy: 0.9685 - f1_m: 0.9524 - precision_m: 0.9630 - recall_m: 0.9430 - val_loss: 0.5690 - val_accuracy: 0.8517 - val_f1_m: 0.7596 - val_precision_m: 0.7757 - val_recall_m: 0.7449\n",
            "Epoch 35/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.0843 - accuracy: 0.9685 - f1_m: 0.9539 - precision_m: 0.9587 - recall_m: 0.9510 - val_loss: 0.5249 - val_accuracy: 0.8431 - val_f1_m: 0.7557 - val_precision_m: 0.7516 - val_recall_m: 0.7610\n",
            "Epoch 36/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.0714 - accuracy: 0.9704 - f1_m: 0.9565 - precision_m: 0.9621 - recall_m: 0.9516 - val_loss: 0.5638 - val_accuracy: 0.8310 - val_f1_m: 0.7515 - val_precision_m: 0.7070 - val_recall_m: 0.8060\n",
            "Epoch 37/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.0620 - accuracy: 0.9779 - f1_m: 0.9682 - precision_m: 0.9609 - recall_m: 0.9765 - val_loss: 0.5747 - val_accuracy: 0.8431 - val_f1_m: 0.7579 - val_precision_m: 0.7684 - val_recall_m: 0.7511\n",
            "Epoch 38/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.0463 - accuracy: 0.9847 - f1_m: 0.9778 - precision_m: 0.9756 - recall_m: 0.9801 - val_loss: 0.5452 - val_accuracy: 0.8621 - val_f1_m: 0.7850 - val_precision_m: 0.8019 - val_recall_m: 0.7709\n",
            "Epoch 39/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.0599 - accuracy: 0.9746 - f1_m: 0.9613 - precision_m: 0.9643 - recall_m: 0.9588 - val_loss: 0.5847 - val_accuracy: 0.8345 - val_f1_m: 0.7628 - val_precision_m: 0.7107 - val_recall_m: 0.8264\n",
            "Epoch 40/40\n",
            "19/19 [==============================] - 24s 1s/step - loss: 0.0744 - accuracy: 0.9733 - f1_m: 0.9616 - precision_m: 0.9545 - recall_m: 0.9716 - val_loss: 0.6194 - val_accuracy: 0.8431 - val_f1_m: 0.7637 - val_precision_m: 0.7729 - val_recall_m: 0.7646\n",
            "\n",
            "Testing word sequence f1_score:  0.7536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Vly187xmH3R",
        "outputId": "520abe99-680d-4c70-e816-f4ab0e7a3e39"
      },
      "source": [
        "y_pred_test = model.predict_classes(xseq_test)\n",
        "y_pred_test = convert_prob(y_pred_test)\n",
        "y_pred_test = np.array(y_pred_test).tolist()\n",
        "write_pred(df_test[\"id\"], y_pred_test, \"test-lstm-0,7768.json\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "save finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFOnca4gT3QP",
        "outputId": "a89f9e7f-3db5-404a-a2ca-6907d2752d22"
      },
      "source": [
        "\"\"\"\n",
        "CNN+RNN 串联\n",
        "\"\"\"\n",
        "# we don't want the model to overwrite, dont we?\n",
        "keras.backend.clear_session()\n",
        "\n",
        "from keras.layers import BatchNormalization\n",
        "import tensorflow as tf\n",
        "from keras.layers import Convolution1D, Flatten, Dropout, MaxPool1D, GlobalAveragePooling1D\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(Embedding(nb_words,embed_dim,input_length=max_seq_len, weights=[embedding_matrix], trainable=False))\n",
        "model.add(Convolution1D(256, 3, padding='same', strides = 1))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPool1D(pool_size=2))\n",
        "model.add(GRU(256, dropout=0.2, recurrent_dropout=0.1, return_sequences = True))\n",
        "model.add(GRU(256, dropout=0.2, recurrent_dropout=0.1))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', f1_m, precision_m, recall_m])\n",
        "\n",
        "# training\n",
        "model.fit(xseq_train, y_train, epochs=num_epochs, verbose=True, validation_data=(xseq_dev, y_dev), batch_size=256)\n",
        "loss, accuracy, f1_score, precision, recall = model.evaluate(xseq_dev, y_dev, verbose=False)\n",
        "print(\"\\nTesting word sequence f1_score:  {:.4f}\".format(f1_score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 11, 300)           1525800   \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 11, 256)           230656    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 11, 256)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 5, 256)            0         \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 5, 256)            394752    \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 256)               394752    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 2,546,217\n",
            "Trainable params: 1,020,417\n",
            "Non-trainable params: 1,525,800\n",
            "_________________________________________________________________\n",
            "Epoch 1/40\n",
            "19/19 [==============================] - 12s 363ms/step - loss: 0.6186 - accuracy: 0.6795 - f1_m: 0.1810 - precision_m: 0.4006 - recall_m: 0.1471 - val_loss: 0.3813 - val_accuracy: 0.8138 - val_f1_m: 0.6806 - val_precision_m: 0.6748 - val_recall_m: 0.6937\n",
            "Epoch 2/40\n",
            "19/19 [==============================] - 6s 316ms/step - loss: 0.3913 - accuracy: 0.8218 - f1_m: 0.7316 - precision_m: 0.7582 - recall_m: 0.7134 - val_loss: 0.3489 - val_accuracy: 0.8448 - val_f1_m: 0.7128 - val_precision_m: 0.7522 - val_recall_m: 0.6856\n",
            "Epoch 3/40\n",
            "19/19 [==============================] - 6s 311ms/step - loss: 0.3344 - accuracy: 0.8571 - f1_m: 0.7873 - precision_m: 0.8245 - recall_m: 0.7592 - val_loss: 0.4014 - val_accuracy: 0.8293 - val_f1_m: 0.6750 - val_precision_m: 0.8691 - val_recall_m: 0.5584\n",
            "Epoch 4/40\n",
            "19/19 [==============================] - 6s 310ms/step - loss: 0.2886 - accuracy: 0.8747 - f1_m: 0.8048 - precision_m: 0.8562 - recall_m: 0.7821 - val_loss: 0.3448 - val_accuracy: 0.8517 - val_f1_m: 0.7454 - val_precision_m: 0.8463 - val_recall_m: 0.6776\n",
            "Epoch 5/40\n",
            "19/19 [==============================] - 6s 310ms/step - loss: 0.2257 - accuracy: 0.9070 - f1_m: 0.8579 - precision_m: 0.9024 - recall_m: 0.8324 - val_loss: 0.3429 - val_accuracy: 0.8517 - val_f1_m: 0.7735 - val_precision_m: 0.7477 - val_recall_m: 0.8060\n",
            "Epoch 6/40\n",
            "19/19 [==============================] - 6s 313ms/step - loss: 0.1176 - accuracy: 0.9591 - f1_m: 0.9405 - precision_m: 0.9367 - recall_m: 0.9469 - val_loss: 0.4557 - val_accuracy: 0.8552 - val_f1_m: 0.7737 - val_precision_m: 0.8277 - val_recall_m: 0.7375\n",
            "Epoch 7/40\n",
            "19/19 [==============================] - 6s 316ms/step - loss: 0.0551 - accuracy: 0.9796 - f1_m: 0.9700 - precision_m: 0.9672 - recall_m: 0.9730 - val_loss: 0.5593 - val_accuracy: 0.8483 - val_f1_m: 0.7716 - val_precision_m: 0.8177 - val_recall_m: 0.7424\n",
            "Epoch 8/40\n",
            "19/19 [==============================] - 6s 309ms/step - loss: 0.0660 - accuracy: 0.9796 - f1_m: 0.9699 - precision_m: 0.9709 - recall_m: 0.9699 - val_loss: 0.4678 - val_accuracy: 0.8431 - val_f1_m: 0.7738 - val_precision_m: 0.7201 - val_recall_m: 0.8413\n",
            "Epoch 9/40\n",
            "19/19 [==============================] - 6s 309ms/step - loss: 0.0461 - accuracy: 0.9843 - f1_m: 0.9766 - precision_m: 0.9677 - recall_m: 0.9867 - val_loss: 0.6830 - val_accuracy: 0.8586 - val_f1_m: 0.7654 - val_precision_m: 0.8482 - val_recall_m: 0.7048\n",
            "Epoch 10/40\n",
            "19/19 [==============================] - 6s 309ms/step - loss: 0.0417 - accuracy: 0.9855 - f1_m: 0.9788 - precision_m: 0.9814 - recall_m: 0.9775 - val_loss: 0.5265 - val_accuracy: 0.8621 - val_f1_m: 0.7858 - val_precision_m: 0.8492 - val_recall_m: 0.7468\n",
            "Epoch 11/40\n",
            "19/19 [==============================] - 6s 311ms/step - loss: 0.0654 - accuracy: 0.9761 - f1_m: 0.9654 - precision_m: 0.9760 - recall_m: 0.9573 - val_loss: 0.7442 - val_accuracy: 0.8500 - val_f1_m: 0.7337 - val_precision_m: 0.8713 - val_recall_m: 0.6393\n",
            "Epoch 12/40\n",
            "19/19 [==============================] - 6s 312ms/step - loss: 0.0408 - accuracy: 0.9864 - f1_m: 0.9799 - precision_m: 0.9871 - recall_m: 0.9738 - val_loss: 0.5177 - val_accuracy: 0.8603 - val_f1_m: 0.7970 - val_precision_m: 0.8097 - val_recall_m: 0.7881\n",
            "Epoch 13/40\n",
            "19/19 [==============================] - 6s 311ms/step - loss: 0.0248 - accuracy: 0.9935 - f1_m: 0.9908 - precision_m: 0.9913 - recall_m: 0.9904 - val_loss: 0.5878 - val_accuracy: 0.8552 - val_f1_m: 0.7900 - val_precision_m: 0.8254 - val_recall_m: 0.7659\n",
            "Epoch 14/40\n",
            "19/19 [==============================] - 6s 311ms/step - loss: 0.0177 - accuracy: 0.9946 - f1_m: 0.9919 - precision_m: 0.9912 - recall_m: 0.9928 - val_loss: 0.5948 - val_accuracy: 0.8569 - val_f1_m: 0.7828 - val_precision_m: 0.8341 - val_recall_m: 0.7449\n",
            "Epoch 15/40\n",
            "19/19 [==============================] - 6s 309ms/step - loss: 0.0143 - accuracy: 0.9961 - f1_m: 0.9941 - precision_m: 0.9956 - recall_m: 0.9927 - val_loss: 0.5385 - val_accuracy: 0.8672 - val_f1_m: 0.8081 - val_precision_m: 0.8286 - val_recall_m: 0.7918\n",
            "Epoch 16/40\n",
            "19/19 [==============================] - 6s 311ms/step - loss: 0.0245 - accuracy: 0.9928 - f1_m: 0.9890 - precision_m: 0.9850 - recall_m: 0.9933 - val_loss: 0.5937 - val_accuracy: 0.8586 - val_f1_m: 0.7924 - val_precision_m: 0.8136 - val_recall_m: 0.7758\n",
            "Epoch 17/40\n",
            "19/19 [==============================] - 6s 311ms/step - loss: 0.0130 - accuracy: 0.9970 - f1_m: 0.9955 - precision_m: 0.9937 - recall_m: 0.9973 - val_loss: 0.6385 - val_accuracy: 0.8466 - val_f1_m: 0.7705 - val_precision_m: 0.7729 - val_recall_m: 0.7727\n",
            "Epoch 18/40\n",
            "19/19 [==============================] - 6s 313ms/step - loss: 0.0160 - accuracy: 0.9956 - f1_m: 0.9937 - precision_m: 0.9915 - recall_m: 0.9960 - val_loss: 0.6955 - val_accuracy: 0.8552 - val_f1_m: 0.7964 - val_precision_m: 0.7422 - val_recall_m: 0.8635\n",
            "Epoch 19/40\n",
            "19/19 [==============================] - 6s 310ms/step - loss: 0.0149 - accuracy: 0.9950 - f1_m: 0.9923 - precision_m: 0.9878 - recall_m: 0.9969 - val_loss: 0.6332 - val_accuracy: 0.8569 - val_f1_m: 0.7971 - val_precision_m: 0.8137 - val_recall_m: 0.7857\n",
            "Epoch 20/40\n",
            "19/19 [==============================] - 6s 313ms/step - loss: 0.0188 - accuracy: 0.9968 - f1_m: 0.9950 - precision_m: 0.9928 - recall_m: 0.9973 - val_loss: 0.6649 - val_accuracy: 0.8500 - val_f1_m: 0.7802 - val_precision_m: 0.7629 - val_recall_m: 0.8048\n",
            "Epoch 21/40\n",
            "19/19 [==============================] - 6s 311ms/step - loss: 0.0279 - accuracy: 0.9899 - f1_m: 0.9856 - precision_m: 0.9778 - recall_m: 0.9939 - val_loss: 0.6074 - val_accuracy: 0.8638 - val_f1_m: 0.7933 - val_precision_m: 0.7878 - val_recall_m: 0.8024\n",
            "Epoch 22/40\n",
            "19/19 [==============================] - 6s 309ms/step - loss: 0.0221 - accuracy: 0.9931 - f1_m: 0.9898 - precision_m: 0.9840 - recall_m: 0.9958 - val_loss: 0.8089 - val_accuracy: 0.8517 - val_f1_m: 0.7677 - val_precision_m: 0.8282 - val_recall_m: 0.7208\n",
            "Epoch 23/40\n",
            "19/19 [==============================] - 6s 309ms/step - loss: 0.0150 - accuracy: 0.9963 - f1_m: 0.9945 - precision_m: 0.9927 - recall_m: 0.9963 - val_loss: 0.7909 - val_accuracy: 0.8500 - val_f1_m: 0.7549 - val_precision_m: 0.8327 - val_recall_m: 0.6961\n",
            "Epoch 24/40\n",
            "19/19 [==============================] - 6s 309ms/step - loss: 0.0280 - accuracy: 0.9904 - f1_m: 0.9854 - precision_m: 0.9892 - recall_m: 0.9820 - val_loss: 0.6746 - val_accuracy: 0.8500 - val_f1_m: 0.7788 - val_precision_m: 0.7500 - val_recall_m: 0.8153\n",
            "Epoch 25/40\n",
            "19/19 [==============================] - 6s 310ms/step - loss: 0.0122 - accuracy: 0.9969 - f1_m: 0.9952 - precision_m: 0.9951 - recall_m: 0.9953 - val_loss: 0.7465 - val_accuracy: 0.8483 - val_f1_m: 0.7799 - val_precision_m: 0.7806 - val_recall_m: 0.7844\n",
            "Epoch 26/40\n",
            "19/19 [==============================] - 6s 312ms/step - loss: 0.0131 - accuracy: 0.9968 - f1_m: 0.9954 - precision_m: 0.9933 - recall_m: 0.9975 - val_loss: 0.7024 - val_accuracy: 0.8534 - val_f1_m: 0.7780 - val_precision_m: 0.8375 - val_recall_m: 0.7332\n",
            "Epoch 27/40\n",
            "19/19 [==============================] - 6s 313ms/step - loss: 0.0149 - accuracy: 0.9962 - f1_m: 0.9944 - precision_m: 0.9937 - recall_m: 0.9952 - val_loss: 0.6766 - val_accuracy: 0.8552 - val_f1_m: 0.7822 - val_precision_m: 0.8057 - val_recall_m: 0.7641\n",
            "Epoch 28/40\n",
            "19/19 [==============================] - 6s 311ms/step - loss: 0.0117 - accuracy: 0.9967 - f1_m: 0.9951 - precision_m: 0.9944 - recall_m: 0.9959 - val_loss: 0.8542 - val_accuracy: 0.8431 - val_f1_m: 0.7642 - val_precision_m: 0.7743 - val_recall_m: 0.7610\n",
            "Epoch 29/40\n",
            "19/19 [==============================] - 6s 311ms/step - loss: 0.0133 - accuracy: 0.9957 - f1_m: 0.9938 - precision_m: 0.9915 - recall_m: 0.9962 - val_loss: 0.7317 - val_accuracy: 0.8483 - val_f1_m: 0.7788 - val_precision_m: 0.7814 - val_recall_m: 0.7801\n",
            "Epoch 30/40\n",
            "19/19 [==============================] - 6s 311ms/step - loss: 0.0088 - accuracy: 0.9977 - f1_m: 0.9966 - precision_m: 0.9958 - recall_m: 0.9974 - val_loss: 0.8175 - val_accuracy: 0.8603 - val_f1_m: 0.8061 - val_precision_m: 0.7807 - val_recall_m: 0.8357\n",
            "Epoch 31/40\n",
            "19/19 [==============================] - 6s 311ms/step - loss: 0.0135 - accuracy: 0.9962 - f1_m: 0.9945 - precision_m: 0.9922 - recall_m: 0.9969 - val_loss: 0.7789 - val_accuracy: 0.8621 - val_f1_m: 0.7969 - val_precision_m: 0.7943 - val_recall_m: 0.8048\n",
            "Epoch 32/40\n",
            "19/19 [==============================] - 6s 311ms/step - loss: 0.0073 - accuracy: 0.9976 - f1_m: 0.9963 - precision_m: 0.9963 - recall_m: 0.9964 - val_loss: 0.8765 - val_accuracy: 0.8655 - val_f1_m: 0.8061 - val_precision_m: 0.8080 - val_recall_m: 0.8079\n",
            "Epoch 33/40\n",
            "19/19 [==============================] - 6s 310ms/step - loss: 0.0079 - accuracy: 0.9967 - f1_m: 0.9952 - precision_m: 0.9955 - recall_m: 0.9949 - val_loss: 0.9339 - val_accuracy: 0.8431 - val_f1_m: 0.7296 - val_precision_m: 0.8429 - val_recall_m: 0.6480\n",
            "Epoch 34/40\n",
            "19/19 [==============================] - 6s 308ms/step - loss: 0.0295 - accuracy: 0.9908 - f1_m: 0.9865 - precision_m: 0.9890 - recall_m: 0.9844 - val_loss: 0.8590 - val_accuracy: 0.8362 - val_f1_m: 0.7153 - val_precision_m: 0.8737 - val_recall_m: 0.6109\n",
            "Epoch 35/40\n",
            "19/19 [==============================] - 6s 326ms/step - loss: 0.0273 - accuracy: 0.9899 - f1_m: 0.9865 - precision_m: 0.9898 - recall_m: 0.9835 - val_loss: 0.8424 - val_accuracy: 0.8414 - val_f1_m: 0.7536 - val_precision_m: 0.8237 - val_recall_m: 0.6998\n",
            "Epoch 36/40\n",
            "19/19 [==============================] - 7s 389ms/step - loss: 0.0199 - accuracy: 0.9939 - f1_m: 0.9915 - precision_m: 0.9955 - recall_m: 0.9877 - val_loss: 0.8830 - val_accuracy: 0.8431 - val_f1_m: 0.7277 - val_precision_m: 0.8780 - val_recall_m: 0.6269\n",
            "Epoch 37/40\n",
            "19/19 [==============================] - 6s 315ms/step - loss: 0.0260 - accuracy: 0.9918 - f1_m: 0.9876 - precision_m: 0.9920 - recall_m: 0.9836 - val_loss: 0.6767 - val_accuracy: 0.8603 - val_f1_m: 0.7693 - val_precision_m: 0.8442 - val_recall_m: 0.7122\n",
            "Epoch 38/40\n",
            "19/19 [==============================] - 6s 309ms/step - loss: 0.0288 - accuracy: 0.9895 - f1_m: 0.9852 - precision_m: 0.9894 - recall_m: 0.9816 - val_loss: 0.6377 - val_accuracy: 0.8552 - val_f1_m: 0.7613 - val_precision_m: 0.8145 - val_recall_m: 0.7196\n",
            "Epoch 39/40\n",
            "19/19 [==============================] - 6s 309ms/step - loss: 0.0117 - accuracy: 0.9960 - f1_m: 0.9942 - precision_m: 0.9963 - recall_m: 0.9922 - val_loss: 0.8292 - val_accuracy: 0.8517 - val_f1_m: 0.7642 - val_precision_m: 0.8387 - val_recall_m: 0.7091\n",
            "Epoch 40/40\n",
            "19/19 [==============================] - 6s 309ms/step - loss: 0.0087 - accuracy: 0.9965 - f1_m: 0.9946 - precision_m: 0.9965 - recall_m: 0.9928 - val_loss: 0.8612 - val_accuracy: 0.8534 - val_f1_m: 0.7864 - val_precision_m: 0.8052 - val_recall_m: 0.7708\n",
            "\n",
            "Testing word sequence f1_score:  0.7234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0dqjzmFT3NT",
        "outputId": "be35f301-d603-4e96-c77f-0fcdee8a82ba"
      },
      "source": [
        "y_pred_test = model.predict(xseq_test)\n",
        "y_pred_test = convert_prob(y_pred_test)\n",
        "y_pred_test = np.array(y_pred_test).tolist()\n",
        "write_pred(df_test[\"id\"], y_pred_test, \"test-cnn+rnn.json\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "save finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tADmePwkgRl2"
      },
      "source": [
        "# y_labels = pd.DataFrame(y_train)\n",
        "# y_labels = list(y_labels.value_counts().index)\n",
        "# num_labels = len(y_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxeD0r-2T3KW",
        "outputId": "cf1f89ca-596a-4f9d-8ef3-59456301cec8"
      },
      "source": [
        "\"\"\"\n",
        "CNN+RNN 并联\n",
        "\"\"\"\n",
        "# we don't want the model to overwrite, dont we?\n",
        "keras.backend.clear_session()\n",
        "\n",
        "# y_labels = pd.Dataframe(y_train)\n",
        "# y_labels = list(y_labels.value_counts().index)\n",
        "# num_labels = len(y_labels)\n",
        "\n",
        "from keras.layers import BatchNormalization\n",
        "import tensorflow as tf\n",
        "from keras.layers import Convolution1D, Flatten, Dropout, MaxPool1D, GlobalAveragePooling1D\n",
        "from keras.layers import Dense, Embedding, Activation, merge, Input, Lambda, Reshape\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Sequential, Model\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(Embedding(nb_words,embed_dim,input_length=max_seq_len, weights=[embedding_matrix],trainable=False))\n",
        "\n",
        "main_input = Input(shape=(11,), dtype='float64')\n",
        "embed = Embedding(nb_words,embed_dim, input_length=max_seq_len, weights=[embedding_matrix], trainable=False)(main_input)\n",
        "cnn = Convolution1D(256, 3, padding='same', strides = 1, activation='relu')(embed)\n",
        "cnn = MaxPool1D(pool_size=4)(cnn)\n",
        "cnn = Flatten()(cnn)\n",
        "cnn = Dense(256)(cnn)\n",
        "rnn = Bidirectional(GRU(256, dropout=0.2, recurrent_dropout=0.1))(embed)\n",
        "rnn = Dense(256)(rnn)\n",
        "con = concatenate([cnn,rnn], axis=-1)\n",
        "main_output = Dense(1, activation='sigmoid')(con)\n",
        "model = Model(inputs = main_input, outputs = main_output)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', f1_m, precision_m, recall_m])\n",
        "\n",
        "# training\n",
        "model.fit(xseq_train, y_train, epochs=num_epochs, verbose=True, validation_data=(xseq_dev, y_dev), batch_size=256)\n",
        "loss, accuracy, f1_score, precision, recall = model.evaluate(xseq_dev, y_dev, verbose=False)\n",
        "print(\"\\nTesting word sequence f1_score:  {:.4f}\".format(f1_score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 11)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 11, 300)      1525800     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 11, 256)      230656      embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D)    (None, 2, 256)       0           conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 512)          0           max_pooling1d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   (None, 512)          857088      embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          131328      flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          131328      bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 512)          0           dense[0][0]                      \n",
            "                                                                 dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            513         concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 2,876,713\n",
            "Trainable params: 1,350,913\n",
            "Non-trainable params: 1,525,800\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/40\n",
            "19/19 [==============================] - 17s 617ms/step - loss: 0.6118 - accuracy: 0.6981 - f1_m: 0.3254 - precision_m: 0.5114 - recall_m: 0.2980 - val_loss: 0.4042 - val_accuracy: 0.8224 - val_f1_m: 0.6472 - val_precision_m: 0.7899 - val_recall_m: 0.5504\n",
            "Epoch 2/40\n",
            "19/19 [==============================] - 11s 571ms/step - loss: 0.3922 - accuracy: 0.8211 - f1_m: 0.7149 - precision_m: 0.7614 - recall_m: 0.6800 - val_loss: 0.3721 - val_accuracy: 0.8362 - val_f1_m: 0.6977 - val_precision_m: 0.8027 - val_recall_m: 0.6282\n",
            "Epoch 3/40\n",
            "19/19 [==============================] - 11s 570ms/step - loss: 0.3321 - accuracy: 0.8555 - f1_m: 0.7701 - precision_m: 0.8404 - recall_m: 0.7248 - val_loss: 0.3617 - val_accuracy: 0.8310 - val_f1_m: 0.6796 - val_precision_m: 0.8398 - val_recall_m: 0.5794\n",
            "Epoch 4/40\n",
            "19/19 [==============================] - 11s 573ms/step - loss: 0.2643 - accuracy: 0.8947 - f1_m: 0.8339 - precision_m: 0.8880 - recall_m: 0.7927 - val_loss: 0.3327 - val_accuracy: 0.8552 - val_f1_m: 0.7444 - val_precision_m: 0.7917 - val_recall_m: 0.7066\n",
            "Epoch 5/40\n",
            "19/19 [==============================] - 11s 577ms/step - loss: 0.1604 - accuracy: 0.9433 - f1_m: 0.9165 - precision_m: 0.9260 - recall_m: 0.9084 - val_loss: 0.3387 - val_accuracy: 0.8500 - val_f1_m: 0.7451 - val_precision_m: 0.7714 - val_recall_m: 0.7270\n",
            "Epoch 6/40\n",
            "19/19 [==============================] - 11s 577ms/step - loss: 0.1089 - accuracy: 0.9725 - f1_m: 0.9596 - precision_m: 0.9627 - recall_m: 0.9585 - val_loss: 0.3592 - val_accuracy: 0.8586 - val_f1_m: 0.7546 - val_precision_m: 0.8252 - val_recall_m: 0.6992\n",
            "Epoch 7/40\n",
            "19/19 [==============================] - 11s 574ms/step - loss: 0.0627 - accuracy: 0.9852 - f1_m: 0.9782 - precision_m: 0.9902 - recall_m: 0.9670 - val_loss: 0.4047 - val_accuracy: 0.8431 - val_f1_m: 0.7200 - val_precision_m: 0.8174 - val_recall_m: 0.6474\n",
            "Epoch 8/40\n",
            "19/19 [==============================] - 11s 571ms/step - loss: 0.0357 - accuracy: 0.9950 - f1_m: 0.9927 - precision_m: 0.9950 - recall_m: 0.9904 - val_loss: 0.3961 - val_accuracy: 0.8500 - val_f1_m: 0.7365 - val_precision_m: 0.8035 - val_recall_m: 0.6813\n",
            "Epoch 9/40\n",
            "19/19 [==============================] - 11s 569ms/step - loss: 0.0288 - accuracy: 0.9977 - f1_m: 0.9967 - precision_m: 0.9956 - recall_m: 0.9978 - val_loss: 0.4306 - val_accuracy: 0.8448 - val_f1_m: 0.7225 - val_precision_m: 0.7945 - val_recall_m: 0.6652\n",
            "Epoch 10/40\n",
            "19/19 [==============================] - 11s 575ms/step - loss: 0.0192 - accuracy: 0.9972 - f1_m: 0.9959 - precision_m: 0.9940 - recall_m: 0.9978 - val_loss: 0.4426 - val_accuracy: 0.8603 - val_f1_m: 0.7497 - val_precision_m: 0.8177 - val_recall_m: 0.6937\n",
            "Epoch 11/40\n",
            "19/19 [==============================] - 11s 573ms/step - loss: 0.0391 - accuracy: 0.9962 - f1_m: 0.9943 - precision_m: 0.9951 - recall_m: 0.9935 - val_loss: 0.4530 - val_accuracy: 0.8569 - val_f1_m: 0.7602 - val_precision_m: 0.8017 - val_recall_m: 0.7270\n",
            "Epoch 12/40\n",
            "19/19 [==============================] - 11s 574ms/step - loss: 0.0153 - accuracy: 0.9974 - f1_m: 0.9960 - precision_m: 0.9952 - recall_m: 0.9968 - val_loss: 0.4618 - val_accuracy: 0.8586 - val_f1_m: 0.7643 - val_precision_m: 0.7740 - val_recall_m: 0.7573\n",
            "Epoch 13/40\n",
            "19/19 [==============================] - 11s 571ms/step - loss: 0.0166 - accuracy: 0.9972 - f1_m: 0.9959 - precision_m: 0.9955 - recall_m: 0.9964 - val_loss: 0.4949 - val_accuracy: 0.8603 - val_f1_m: 0.7730 - val_precision_m: 0.8063 - val_recall_m: 0.7474\n",
            "Epoch 14/40\n",
            "19/19 [==============================] - 11s 569ms/step - loss: 0.0113 - accuracy: 0.9968 - f1_m: 0.9955 - precision_m: 0.9963 - recall_m: 0.9947 - val_loss: 0.4805 - val_accuracy: 0.8638 - val_f1_m: 0.7739 - val_precision_m: 0.7927 - val_recall_m: 0.7579\n",
            "Epoch 15/40\n",
            "19/19 [==============================] - 11s 570ms/step - loss: 0.0137 - accuracy: 0.9982 - f1_m: 0.9975 - precision_m: 0.9976 - recall_m: 0.9973 - val_loss: 0.5201 - val_accuracy: 0.8534 - val_f1_m: 0.7397 - val_precision_m: 0.8119 - val_recall_m: 0.6813\n",
            "Epoch 16/40\n",
            "19/19 [==============================] - 11s 567ms/step - loss: 0.0101 - accuracy: 0.9991 - f1_m: 0.9986 - precision_m: 0.9988 - recall_m: 0.9984 - val_loss: 0.4869 - val_accuracy: 0.8638 - val_f1_m: 0.7617 - val_precision_m: 0.8039 - val_recall_m: 0.7264\n",
            "Epoch 17/40\n",
            "19/19 [==============================] - 11s 568ms/step - loss: 0.0112 - accuracy: 0.9986 - f1_m: 0.9980 - precision_m: 0.9984 - recall_m: 0.9975 - val_loss: 0.5503 - val_accuracy: 0.8552 - val_f1_m: 0.7361 - val_precision_m: 0.8350 - val_recall_m: 0.6628\n",
            "Epoch 18/40\n",
            "19/19 [==============================] - 11s 568ms/step - loss: 0.0138 - accuracy: 0.9975 - f1_m: 0.9964 - precision_m: 0.9965 - recall_m: 0.9963 - val_loss: 0.4984 - val_accuracy: 0.8707 - val_f1_m: 0.7907 - val_precision_m: 0.7786 - val_recall_m: 0.8061\n",
            "Epoch 19/40\n",
            "19/19 [==============================] - 11s 568ms/step - loss: 0.0135 - accuracy: 0.9975 - f1_m: 0.9966 - precision_m: 0.9953 - recall_m: 0.9979 - val_loss: 0.5056 - val_accuracy: 0.8759 - val_f1_m: 0.7964 - val_precision_m: 0.7855 - val_recall_m: 0.8104\n",
            "Epoch 20/40\n",
            "19/19 [==============================] - 11s 571ms/step - loss: 0.0089 - accuracy: 0.9978 - f1_m: 0.9971 - precision_m: 0.9955 - recall_m: 0.9987 - val_loss: 0.4956 - val_accuracy: 0.8672 - val_f1_m: 0.7755 - val_precision_m: 0.7808 - val_recall_m: 0.7727\n",
            "Epoch 21/40\n",
            "19/19 [==============================] - 11s 570ms/step - loss: 0.0110 - accuracy: 0.9968 - f1_m: 0.9954 - precision_m: 0.9952 - recall_m: 0.9957 - val_loss: 0.5166 - val_accuracy: 0.8517 - val_f1_m: 0.7749 - val_precision_m: 0.7288 - val_recall_m: 0.8314\n",
            "Epoch 22/40\n",
            "19/19 [==============================] - 11s 572ms/step - loss: 0.0129 - accuracy: 0.9985 - f1_m: 0.9979 - precision_m: 0.9982 - recall_m: 0.9977 - val_loss: 0.5840 - val_accuracy: 0.8500 - val_f1_m: 0.7336 - val_precision_m: 0.8389 - val_recall_m: 0.6535\n",
            "Epoch 23/40\n",
            "19/19 [==============================] - 11s 572ms/step - loss: 0.0182 - accuracy: 0.9972 - f1_m: 0.9960 - precision_m: 0.9964 - recall_m: 0.9956 - val_loss: 0.5311 - val_accuracy: 0.8569 - val_f1_m: 0.7687 - val_precision_m: 0.7192 - val_recall_m: 0.8345\n",
            "Epoch 24/40\n",
            "19/19 [==============================] - 11s 569ms/step - loss: 0.0137 - accuracy: 0.9974 - f1_m: 0.9962 - precision_m: 0.9959 - recall_m: 0.9965 - val_loss: 0.4995 - val_accuracy: 0.8707 - val_f1_m: 0.7925 - val_precision_m: 0.7738 - val_recall_m: 0.8147\n",
            "Epoch 25/40\n",
            "19/19 [==============================] - 11s 574ms/step - loss: 0.0170 - accuracy: 0.9968 - f1_m: 0.9954 - precision_m: 0.9938 - recall_m: 0.9970 - val_loss: 0.6491 - val_accuracy: 0.8362 - val_f1_m: 0.7042 - val_precision_m: 0.8482 - val_recall_m: 0.6047\n",
            "Epoch 26/40\n",
            "19/19 [==============================] - 11s 572ms/step - loss: 0.0213 - accuracy: 0.9978 - f1_m: 0.9969 - precision_m: 0.9959 - recall_m: 0.9980 - val_loss: 0.4956 - val_accuracy: 0.8672 - val_f1_m: 0.7918 - val_precision_m: 0.7849 - val_recall_m: 0.8036\n",
            "Epoch 27/40\n",
            "19/19 [==============================] - 11s 572ms/step - loss: 0.0168 - accuracy: 0.9966 - f1_m: 0.9950 - precision_m: 0.9958 - recall_m: 0.9942 - val_loss: 0.4819 - val_accuracy: 0.8621 - val_f1_m: 0.7845 - val_precision_m: 0.8047 - val_recall_m: 0.7721\n",
            "Epoch 28/40\n",
            "19/19 [==============================] - 11s 569ms/step - loss: 0.0144 - accuracy: 0.9982 - f1_m: 0.9973 - precision_m: 0.9968 - recall_m: 0.9979 - val_loss: 0.5896 - val_accuracy: 0.8448 - val_f1_m: 0.7311 - val_precision_m: 0.8220 - val_recall_m: 0.6628\n",
            "Epoch 29/40\n",
            "19/19 [==============================] - 11s 575ms/step - loss: 0.0162 - accuracy: 0.9977 - f1_m: 0.9968 - precision_m: 0.9963 - recall_m: 0.9973 - val_loss: 0.5266 - val_accuracy: 0.8534 - val_f1_m: 0.7355 - val_precision_m: 0.8014 - val_recall_m: 0.6819\n",
            "Epoch 30/40\n",
            "19/19 [==============================] - 11s 570ms/step - loss: 0.0207 - accuracy: 0.9981 - f1_m: 0.9973 - precision_m: 0.9970 - recall_m: 0.9976 - val_loss: 0.4791 - val_accuracy: 0.8603 - val_f1_m: 0.7816 - val_precision_m: 0.8039 - val_recall_m: 0.7671\n",
            "Epoch 31/40\n",
            "19/19 [==============================] - 11s 579ms/step - loss: 0.0133 - accuracy: 0.9978 - f1_m: 0.9969 - precision_m: 0.9965 - recall_m: 0.9973 - val_loss: 0.5001 - val_accuracy: 0.8517 - val_f1_m: 0.7452 - val_precision_m: 0.7923 - val_recall_m: 0.7060\n",
            "Epoch 32/40\n",
            "19/19 [==============================] - 11s 573ms/step - loss: 0.0163 - accuracy: 0.9964 - f1_m: 0.9947 - precision_m: 0.9972 - recall_m: 0.9922 - val_loss: 0.4806 - val_accuracy: 0.8655 - val_f1_m: 0.7885 - val_precision_m: 0.8081 - val_recall_m: 0.7752\n",
            "Epoch 33/40\n",
            "19/19 [==============================] - 11s 576ms/step - loss: 0.0117 - accuracy: 0.9969 - f1_m: 0.9954 - precision_m: 0.9949 - recall_m: 0.9961 - val_loss: 0.5322 - val_accuracy: 0.8483 - val_f1_m: 0.7285 - val_precision_m: 0.8197 - val_recall_m: 0.6591\n",
            "Epoch 34/40\n",
            "19/19 [==============================] - 11s 589ms/step - loss: 0.0091 - accuracy: 0.9979 - f1_m: 0.9969 - precision_m: 0.9982 - recall_m: 0.9958 - val_loss: 0.4938 - val_accuracy: 0.8672 - val_f1_m: 0.7901 - val_precision_m: 0.7901 - val_recall_m: 0.7956\n",
            "Epoch 35/40\n",
            "19/19 [==============================] - 11s 578ms/step - loss: 0.0110 - accuracy: 0.9974 - f1_m: 0.9963 - precision_m: 0.9966 - recall_m: 0.9960 - val_loss: 0.5029 - val_accuracy: 0.8707 - val_f1_m: 0.7976 - val_precision_m: 0.7843 - val_recall_m: 0.8159\n",
            "Epoch 36/40\n",
            "19/19 [==============================] - 11s 574ms/step - loss: 0.0148 - accuracy: 0.9975 - f1_m: 0.9963 - precision_m: 0.9970 - recall_m: 0.9957 - val_loss: 0.4968 - val_accuracy: 0.8672 - val_f1_m: 0.7764 - val_precision_m: 0.8031 - val_recall_m: 0.7542\n",
            "Epoch 37/40\n",
            "19/19 [==============================] - 11s 570ms/step - loss: 0.0080 - accuracy: 0.9986 - f1_m: 0.9980 - precision_m: 0.9980 - recall_m: 0.9980 - val_loss: 0.4874 - val_accuracy: 0.8690 - val_f1_m: 0.7975 - val_precision_m: 0.7994 - val_recall_m: 0.8005\n",
            "Epoch 38/40\n",
            "19/19 [==============================] - 11s 570ms/step - loss: 0.0079 - accuracy: 0.9986 - f1_m: 0.9980 - precision_m: 0.9972 - recall_m: 0.9988 - val_loss: 0.4860 - val_accuracy: 0.8672 - val_f1_m: 0.7824 - val_precision_m: 0.8120 - val_recall_m: 0.7591\n",
            "Epoch 39/40\n",
            "19/19 [==============================] - 11s 569ms/step - loss: 0.0116 - accuracy: 0.9980 - f1_m: 0.9970 - precision_m: 0.9979 - recall_m: 0.9961 - val_loss: 0.4789 - val_accuracy: 0.8707 - val_f1_m: 0.7890 - val_precision_m: 0.8076 - val_recall_m: 0.7746\n",
            "Epoch 40/40\n",
            "19/19 [==============================] - 11s 571ms/step - loss: 0.0099 - accuracy: 0.9985 - f1_m: 0.9978 - precision_m: 0.9966 - recall_m: 0.9990 - val_loss: 0.5277 - val_accuracy: 0.8431 - val_f1_m: 0.7664 - val_precision_m: 0.7021 - val_recall_m: 0.8462\n",
            "\n",
            "Testing word sequence f1_score:  0.7323\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfYpvq6ST3Hz",
        "outputId": "386c618c-6b0d-4be6-cf11-f7f00b2b9208"
      },
      "source": [
        "y_pred_test = model.predict(xseq_test)\n",
        "y_pred_test = convert_prob(y_pred_test)\n",
        "y_pred_test = np.array(y_pred_test).tolist()\n",
        "write_pred(df_test[\"id\"], y_pred_test, \"test-cnn rnn 并.json\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "save finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsiEI6pV7OG0",
        "outputId": "48e2368d-2bb0-4c08-c785-83c6de3853c8"
      },
      "source": [
        "#embedding matrix\n",
        "import os, re, csv, math, codecs\n",
        "\n",
        "print('loading word embeddings...')\n",
        "embedding_matrix1 = np.zeros((nb_words, 200))\n",
        "embeddings_index1 = {}\n",
        "f = codecs.open('glove.twitter.27B.200d.txt', encoding='utf-8')\n",
        "\n",
        "from tqdm import tqdm\n",
        "for line in tqdm(f):\n",
        "    values = line.rstrip().rsplit(' ')\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index1[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('found %s word vectors' % len(embeddings_index1))\n",
        "for word, i in word_index.items():\n",
        "    if i >= nb_words:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index1.get(word)\n",
        "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix1[i] = embedding_vector\n",
        "    else:\n",
        "        words_not_found.append(word)\n",
        "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix1, axis=1) == 0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading word embeddings...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1193515it [01:52, 10633.60it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "found 1193514 word vectors\n",
            "number of null word embeddings: 172\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUf3mXnK7OET",
        "outputId": "426b272b-8d5c-49de-a4ad-82071016c453"
      },
      "source": [
        "print(\"sample words not found: \", np.random.choice(words_not_found, 10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample words not found:  ['radicalised' 'mirontschuk' 'conjointly' 'phorphet' 'fasle'\n",
            " 'ultranationalism' 'haltern' 'balkanye' 'mirontschuk' 'breivak']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkV-jIpP7OBW",
        "outputId": "7d5a9e1c-9bbf-4131-ac11-fc10e72cc9dc"
      },
      "source": [
        "\"\"\"\n",
        "TextCNN\n",
        "\"\"\"\n",
        "from keras import backend as K\n",
        "\n",
        "main_input = Input(shape=(11,), dtype='float64')\n",
        "embedder = Embedding(nb_words, 200, input_length=max_seq_len, weights=[embedding_matrix1], trainable=False)\n",
        "embed = embedder(main_input)\n",
        "# cnn1模块，kernel_size = 3\n",
        "conv1_1 = Convolution1D(256, 3, padding='same')(embed)\n",
        "bn1_1 = BatchNormalization()(conv1_1)\n",
        "relu1_1 = Activation('relu')(bn1_1)\n",
        "conv1_2 = Convolution1D(128, 3, padding='same')(relu1_1)\n",
        "bn1_2 = BatchNormalization()(conv1_2)\n",
        "relu1_2 = Activation('relu')(bn1_2)\n",
        "cnn1 = MaxPool1D(pool_size=4)(relu1_2)\n",
        "# cnn2模块，kernel_size = 4\n",
        "conv2_1 = Convolution1D(256, 4, padding='same')(embed)\n",
        "bn2_1 = BatchNormalization()(conv2_1)\n",
        "relu2_1 = Activation('relu')(bn2_1)\n",
        "conv2_2 = Convolution1D(128, 4, padding='same')(relu2_1)\n",
        "bn2_2 = BatchNormalization()(conv2_2)\n",
        "relu2_2 = Activation('relu')(bn2_2)\n",
        "cnn2 = MaxPool1D(pool_size=4)(relu2_2)\n",
        "# cnn3模块，kernel_size = 5\n",
        "conv3_1 = Convolution1D(256, 5, padding='same')(embed)\n",
        "bn3_1 = BatchNormalization()(conv3_1)\n",
        "relu3_1 = Activation('relu')(bn3_1)\n",
        "conv3_2 = Convolution1D(128, 5, padding='same')(relu3_1)\n",
        "bn3_2 = BatchNormalization()(conv3_2)\n",
        "relu3_2 = Activation('relu')(bn3_2)\n",
        "cnn3 = MaxPool1D(pool_size=4)(relu3_2)\n",
        "# 拼接三个模块\n",
        "cnn = concatenate([cnn1,cnn2,cnn3], axis=-1)\n",
        "flat = Flatten()(cnn)\n",
        "drop = Dropout(0.5)(flat)\n",
        "fc = Dense(512)(drop)\n",
        "bn = BatchNormalization()(fc)\n",
        "main_output = Dense(1, activation='sigmoid')(bn)\n",
        "model = Model(inputs = main_input, outputs = main_output)\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', f1_m, precision_m, recall_m])\n",
        "\n",
        "# training\n",
        "model.fit(xseq_train, y_train, epochs=num_epochs, verbose=True, validation_data=(xseq_dev, y_dev), batch_size=256)\n",
        "loss, accuracy, f1_score, precision, recall = model.evaluate(xseq_dev, y_dev, verbose=False)\n",
        "print(\"\\nTesting word sequence f1_score:  {:.4f}\".format(f1_score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 11)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 11, 200)      1017200     input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 11, 256)      153856      embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 11, 256)      205056      embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 11, 256)      256256      embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 11, 256)      1024        conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 11, 256)      1024        conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 11, 256)      1024        conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 11, 256)      0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 11, 256)      0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 11, 256)      0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 11, 128)      98432       activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 11, 128)      131200      activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 11, 128)      163968      activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 11, 128)      512         conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 11, 128)      512         conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 11, 128)      512         conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 11, 128)      0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 11, 128)      0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 11, 128)      0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1D)  (None, 2, 128)       0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 2, 128)       0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1D)  (None, 2, 128)       0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 2, 384)       0           max_pooling1d_1[0][0]            \n",
            "                                                                 max_pooling1d_2[0][0]            \n",
            "                                                                 max_pooling1d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 768)          0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 768)          0           flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 512)          393728      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 512)          2048        dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1)            513         batch_normalization_6[0][0]      \n",
            "==================================================================================================\n",
            "Total params: 2,426,865\n",
            "Trainable params: 1,406,337\n",
            "Non-trainable params: 1,020,528\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/40\n",
            "19/19 [==============================] - 13s 525ms/step - loss: 0.7558 - accuracy: 0.6485 - f1_m: 0.5731 - precision_m: 0.4892 - recall_m: 0.6974 - val_loss: 0.4744 - val_accuracy: 0.8207 - val_f1_m: 0.7287 - val_precision_m: 0.6735 - val_recall_m: 0.7937\n",
            "Epoch 2/40\n",
            "19/19 [==============================] - 9s 499ms/step - loss: 0.4431 - accuracy: 0.8103 - f1_m: 0.7399 - precision_m: 0.6945 - recall_m: 0.7949 - val_loss: 0.4181 - val_accuracy: 0.8345 - val_f1_m: 0.7438 - val_precision_m: 0.6918 - val_recall_m: 0.8067\n",
            "Epoch 3/40\n",
            "19/19 [==============================] - 10s 505ms/step - loss: 0.2833 - accuracy: 0.8906 - f1_m: 0.8420 - precision_m: 0.8124 - recall_m: 0.8753 - val_loss: 0.3826 - val_accuracy: 0.8224 - val_f1_m: 0.6820 - val_precision_m: 0.7739 - val_recall_m: 0.6196\n",
            "Epoch 4/40\n",
            "19/19 [==============================] - 10s 506ms/step - loss: 0.1969 - accuracy: 0.9238 - f1_m: 0.8912 - precision_m: 0.8771 - recall_m: 0.9079 - val_loss: 0.3849 - val_accuracy: 0.8224 - val_f1_m: 0.6639 - val_precision_m: 0.8315 - val_recall_m: 0.5603\n",
            "Epoch 5/40\n",
            "19/19 [==============================] - 9s 498ms/step - loss: 0.1339 - accuracy: 0.9490 - f1_m: 0.9257 - precision_m: 0.9129 - recall_m: 0.9404 - val_loss: 0.3572 - val_accuracy: 0.8397 - val_f1_m: 0.7156 - val_precision_m: 0.7896 - val_recall_m: 0.6597\n",
            "Epoch 6/40\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 0.1059 - accuracy: 0.9615 - f1_m: 0.9434 - precision_m: 0.9251 - recall_m: 0.9632 - val_loss: 0.3764 - val_accuracy: 0.8483 - val_f1_m: 0.7348 - val_precision_m: 0.7494 - val_recall_m: 0.7270\n",
            "Epoch 7/40\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 0.0967 - accuracy: 0.9684 - f1_m: 0.9538 - precision_m: 0.9424 - recall_m: 0.9659 - val_loss: 0.3920 - val_accuracy: 0.8362 - val_f1_m: 0.6943 - val_precision_m: 0.7810 - val_recall_m: 0.6294\n",
            "Epoch 8/40\n",
            "19/19 [==============================] - 9s 492ms/step - loss: 0.0897 - accuracy: 0.9629 - f1_m: 0.9441 - precision_m: 0.9311 - recall_m: 0.9593 - val_loss: 0.4594 - val_accuracy: 0.8207 - val_f1_m: 0.6570 - val_precision_m: 0.8432 - val_recall_m: 0.5460\n",
            "Epoch 9/40\n",
            "19/19 [==============================] - 9s 497ms/step - loss: 0.1056 - accuracy: 0.9627 - f1_m: 0.9456 - precision_m: 0.9385 - recall_m: 0.9541 - val_loss: 0.4237 - val_accuracy: 0.8483 - val_f1_m: 0.7723 - val_precision_m: 0.7596 - val_recall_m: 0.7912\n",
            "Epoch 10/40\n",
            "19/19 [==============================] - 10s 499ms/step - loss: 0.0925 - accuracy: 0.9695 - f1_m: 0.9555 - precision_m: 0.9417 - recall_m: 0.9712 - val_loss: 0.4574 - val_accuracy: 0.8534 - val_f1_m: 0.7788 - val_precision_m: 0.7461 - val_recall_m: 0.8240\n",
            "Epoch 11/40\n",
            "19/19 [==============================] - 10s 500ms/step - loss: 0.0805 - accuracy: 0.9742 - f1_m: 0.9621 - precision_m: 0.9503 - recall_m: 0.9747 - val_loss: 0.4397 - val_accuracy: 0.8448 - val_f1_m: 0.7492 - val_precision_m: 0.7650 - val_recall_m: 0.7486\n",
            "Epoch 12/40\n",
            "19/19 [==============================] - 10s 502ms/step - loss: 0.0745 - accuracy: 0.9788 - f1_m: 0.9689 - precision_m: 0.9641 - recall_m: 0.9745 - val_loss: 0.4319 - val_accuracy: 0.8483 - val_f1_m: 0.7709 - val_precision_m: 0.7589 - val_recall_m: 0.7875\n",
            "Epoch 13/40\n",
            "19/19 [==============================] - 9s 499ms/step - loss: 0.0578 - accuracy: 0.9792 - f1_m: 0.9699 - precision_m: 0.9705 - recall_m: 0.9698 - val_loss: 0.4839 - val_accuracy: 0.8379 - val_f1_m: 0.7572 - val_precision_m: 0.7045 - val_recall_m: 0.8240\n",
            "Epoch 14/40\n",
            "19/19 [==============================] - 9s 499ms/step - loss: 0.0419 - accuracy: 0.9869 - f1_m: 0.9803 - precision_m: 0.9752 - recall_m: 0.9859 - val_loss: 0.5222 - val_accuracy: 0.8500 - val_f1_m: 0.7900 - val_precision_m: 0.7604 - val_recall_m: 0.8258\n",
            "Epoch 15/40\n",
            "19/19 [==============================] - 10s 501ms/step - loss: 0.0357 - accuracy: 0.9900 - f1_m: 0.9844 - precision_m: 0.9807 - recall_m: 0.9884 - val_loss: 0.4820 - val_accuracy: 0.8483 - val_f1_m: 0.7586 - val_precision_m: 0.7910 - val_recall_m: 0.7363\n",
            "Epoch 16/40\n",
            "19/19 [==============================] - 10s 506ms/step - loss: 0.0305 - accuracy: 0.9915 - f1_m: 0.9874 - precision_m: 0.9854 - recall_m: 0.9894 - val_loss: 0.5124 - val_accuracy: 0.8534 - val_f1_m: 0.7692 - val_precision_m: 0.8081 - val_recall_m: 0.7393\n",
            "Epoch 17/40\n",
            "19/19 [==============================] - 13s 688ms/step - loss: 0.0263 - accuracy: 0.9922 - f1_m: 0.9882 - precision_m: 0.9883 - recall_m: 0.9883 - val_loss: 0.5611 - val_accuracy: 0.8621 - val_f1_m: 0.7904 - val_precision_m: 0.7680 - val_recall_m: 0.8196\n",
            "Epoch 18/40\n",
            "19/19 [==============================] - 10s 529ms/step - loss: 0.0347 - accuracy: 0.9880 - f1_m: 0.9827 - precision_m: 0.9796 - recall_m: 0.9860 - val_loss: 0.6888 - val_accuracy: 0.8345 - val_f1_m: 0.7689 - val_precision_m: 0.6902 - val_recall_m: 0.8722\n",
            "Epoch 19/40\n",
            "19/19 [==============================] - 9s 485ms/step - loss: 0.0336 - accuracy: 0.9877 - f1_m: 0.9816 - precision_m: 0.9825 - recall_m: 0.9809 - val_loss: 0.6704 - val_accuracy: 0.8379 - val_f1_m: 0.7621 - val_precision_m: 0.6964 - val_recall_m: 0.8437\n",
            "Epoch 20/40\n",
            "19/19 [==============================] - 9s 486ms/step - loss: 0.0220 - accuracy: 0.9928 - f1_m: 0.9892 - precision_m: 0.9854 - recall_m: 0.9933 - val_loss: 0.6104 - val_accuracy: 0.8500 - val_f1_m: 0.7677 - val_precision_m: 0.7417 - val_recall_m: 0.8055\n",
            "Epoch 21/40\n",
            "19/19 [==============================] - 9s 482ms/step - loss: 0.0290 - accuracy: 0.9926 - f1_m: 0.9892 - precision_m: 0.9851 - recall_m: 0.9934 - val_loss: 0.6756 - val_accuracy: 0.8500 - val_f1_m: 0.7772 - val_precision_m: 0.7532 - val_recall_m: 0.8085\n",
            "Epoch 22/40\n",
            "19/19 [==============================] - 9s 486ms/step - loss: 0.0328 - accuracy: 0.9909 - f1_m: 0.9858 - precision_m: 0.9802 - recall_m: 0.9919 - val_loss: 0.6627 - val_accuracy: 0.8483 - val_f1_m: 0.7671 - val_precision_m: 0.7369 - val_recall_m: 0.8104\n",
            "Epoch 23/40\n",
            "19/19 [==============================] - 9s 488ms/step - loss: 0.0229 - accuracy: 0.9943 - f1_m: 0.9912 - precision_m: 0.9867 - recall_m: 0.9959 - val_loss: 0.6658 - val_accuracy: 0.8483 - val_f1_m: 0.7704 - val_precision_m: 0.7469 - val_recall_m: 0.8055\n",
            "Epoch 24/40\n",
            "19/19 [==============================] - 9s 485ms/step - loss: 0.0226 - accuracy: 0.9937 - f1_m: 0.9908 - precision_m: 0.9876 - recall_m: 0.9941 - val_loss: 0.6184 - val_accuracy: 0.8500 - val_f1_m: 0.7720 - val_precision_m: 0.7468 - val_recall_m: 0.8048\n",
            "Epoch 25/40\n",
            "19/19 [==============================] - 9s 489ms/step - loss: 0.0165 - accuracy: 0.9983 - f1_m: 0.9973 - precision_m: 0.9969 - recall_m: 0.9977 - val_loss: 0.6717 - val_accuracy: 0.8534 - val_f1_m: 0.7721 - val_precision_m: 0.7401 - val_recall_m: 0.8123\n",
            "Epoch 26/40\n",
            "19/19 [==============================] - 9s 484ms/step - loss: 0.0394 - accuracy: 0.9886 - f1_m: 0.9836 - precision_m: 0.9801 - recall_m: 0.9875 - val_loss: 0.7404 - val_accuracy: 0.8379 - val_f1_m: 0.7377 - val_precision_m: 0.8162 - val_recall_m: 0.6807\n",
            "Epoch 27/40\n",
            "19/19 [==============================] - 9s 489ms/step - loss: 0.0338 - accuracy: 0.9904 - f1_m: 0.9860 - precision_m: 0.9815 - recall_m: 0.9906 - val_loss: 0.6488 - val_accuracy: 0.8517 - val_f1_m: 0.7823 - val_precision_m: 0.7494 - val_recall_m: 0.8246\n",
            "Epoch 28/40\n",
            "19/19 [==============================] - 9s 494ms/step - loss: 0.0243 - accuracy: 0.9938 - f1_m: 0.9911 - precision_m: 0.9895 - recall_m: 0.9927 - val_loss: 0.6117 - val_accuracy: 0.8397 - val_f1_m: 0.7605 - val_precision_m: 0.7538 - val_recall_m: 0.7813\n",
            "Epoch 29/40\n",
            "19/19 [==============================] - 9s 487ms/step - loss: 0.0192 - accuracy: 0.9940 - f1_m: 0.9914 - precision_m: 0.9907 - recall_m: 0.9923 - val_loss: 0.6378 - val_accuracy: 0.8569 - val_f1_m: 0.7750 - val_precision_m: 0.7870 - val_recall_m: 0.7721\n",
            "Epoch 30/40\n",
            "19/19 [==============================] - 9s 485ms/step - loss: 0.0148 - accuracy: 0.9964 - f1_m: 0.9948 - precision_m: 0.9960 - recall_m: 0.9936 - val_loss: 0.6266 - val_accuracy: 0.8534 - val_f1_m: 0.7806 - val_precision_m: 0.7596 - val_recall_m: 0.8128\n",
            "Epoch 31/40\n",
            "19/19 [==============================] - 9s 485ms/step - loss: 0.0207 - accuracy: 0.9968 - f1_m: 0.9953 - precision_m: 0.9945 - recall_m: 0.9961 - val_loss: 0.6392 - val_accuracy: 0.8431 - val_f1_m: 0.7678 - val_precision_m: 0.7527 - val_recall_m: 0.7980\n",
            "Epoch 32/40\n",
            "19/19 [==============================] - 9s 488ms/step - loss: 0.0174 - accuracy: 0.9959 - f1_m: 0.9940 - precision_m: 0.9914 - recall_m: 0.9967 - val_loss: 0.7838 - val_accuracy: 0.8276 - val_f1_m: 0.7559 - val_precision_m: 0.6705 - val_recall_m: 0.8728\n",
            "Epoch 33/40\n",
            "19/19 [==============================] - 9s 490ms/step - loss: 0.0179 - accuracy: 0.9972 - f1_m: 0.9960 - precision_m: 0.9971 - recall_m: 0.9949 - val_loss: 0.6276 - val_accuracy: 0.8517 - val_f1_m: 0.7741 - val_precision_m: 0.7662 - val_recall_m: 0.7881\n",
            "Epoch 34/40\n",
            "19/19 [==============================] - 9s 487ms/step - loss: 0.0235 - accuracy: 0.9944 - f1_m: 0.9915 - precision_m: 0.9907 - recall_m: 0.9923 - val_loss: 0.6316 - val_accuracy: 0.8534 - val_f1_m: 0.7722 - val_precision_m: 0.7615 - val_recall_m: 0.7925\n",
            "Epoch 35/40\n",
            "19/19 [==============================] - 9s 484ms/step - loss: 0.0168 - accuracy: 0.9947 - f1_m: 0.9921 - precision_m: 0.9910 - recall_m: 0.9934 - val_loss: 0.8366 - val_accuracy: 0.8259 - val_f1_m: 0.6962 - val_precision_m: 0.8410 - val_recall_m: 0.5960\n",
            "Epoch 36/40\n",
            "19/19 [==============================] - 9s 490ms/step - loss: 0.0162 - accuracy: 0.9962 - f1_m: 0.9942 - precision_m: 0.9937 - recall_m: 0.9949 - val_loss: 0.7302 - val_accuracy: 0.8448 - val_f1_m: 0.7600 - val_precision_m: 0.7375 - val_recall_m: 0.7900\n",
            "Epoch 37/40\n",
            "19/19 [==============================] - 9s 495ms/step - loss: 0.0151 - accuracy: 0.9946 - f1_m: 0.9916 - precision_m: 0.9894 - recall_m: 0.9939 - val_loss: 0.6773 - val_accuracy: 0.8534 - val_f1_m: 0.7633 - val_precision_m: 0.8038 - val_recall_m: 0.7356\n",
            "Epoch 38/40\n",
            "19/19 [==============================] - 9s 490ms/step - loss: 0.0360 - accuracy: 0.9923 - f1_m: 0.9887 - precision_m: 0.9903 - recall_m: 0.9872 - val_loss: 0.7561 - val_accuracy: 0.8431 - val_f1_m: 0.7668 - val_precision_m: 0.6944 - val_recall_m: 0.8641\n",
            "Epoch 39/40\n",
            "19/19 [==============================] - 9s 493ms/step - loss: 0.0272 - accuracy: 0.9916 - f1_m: 0.9875 - precision_m: 0.9841 - recall_m: 0.9912 - val_loss: 0.8009 - val_accuracy: 0.8241 - val_f1_m: 0.7521 - val_precision_m: 0.6933 - val_recall_m: 0.8320\n",
            "Epoch 40/40\n",
            "19/19 [==============================] - 9s 492ms/step - loss: 0.0356 - accuracy: 0.9891 - f1_m: 0.9841 - precision_m: 0.9878 - recall_m: 0.9806 - val_loss: 0.8470 - val_accuracy: 0.8121 - val_f1_m: 0.7428 - val_precision_m: 0.6522 - val_recall_m: 0.8678\n",
            "\n",
            "Testing word sequence f1_score:  0.7382\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2irIIGiZ7N-H"
      },
      "source": [
        "y_pred_test = model.predict(xseq_test)\n",
        "y_pred_test = convert_prob(y_pred_test)\n",
        "y_pred_test = np.array(y_pred_test).tolist()\n",
        "write_pred(df_test[\"id\"], y_pred_test, \"test-textcnn.json\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}