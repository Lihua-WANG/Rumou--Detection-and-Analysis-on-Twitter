{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bow-seq.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOjJ6d+rKU0j7vWMBVfDq3t"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5caYYNp4zbVU","executionInfo":{"status":"ok","timestamp":1619848362524,"user_tz":-600,"elapsed":987,"user":{"displayName":"LIHUA WANG","photoUrl":"","userId":"14598865299805559806"}},"outputId":"393faade-5497-417b-d4b2-f85c3e6234e4"},"source":["import json\n","import string\n","from keras.preprocessing.text import Tokenizer\n","import pandas as pd\n","import numpy as np\n","import re\n","import nltk\n","import pip\n","from nltk import collections\n","from nltk.tag import pos_tag\n","from nltk.corpus import wordnet\n","from nltk.tokenize import TweetTokenizer\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn import svm, metrics\n","from sklearn.metrics import precision_recall_fscore_support\n","from nltk.stem import LancasterStemmer, WordNetLemmatizer\n","from keras.models import Sequential\n","from keras import layers\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import LSTM\n","from keras import backend as K\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-BrlWl-30x6W","executionInfo":{"status":"ok","timestamp":1619848367661,"user_tz":-600,"elapsed":6115,"user":{"displayName":"LIHUA WANG","photoUrl":"","userId":"14598865299805559806"}},"outputId":"37380a9a-4525-4552-b91d-b0476d7bf2bb"},"source":["def process_data(read_data_filepath, read_label_filepath):\n","  label_map = {'non-rumour': 0, 'rumour': 1}\n","  index_map = {v: k for k, v in label_map.items()}\n","  with open(read_data_filepath, 'r') as f:\n","    source_id = []\n","    text = []\n","    label = [] \n","    for line in f:\n","      line = json.loads(line)\n","      source_id.append(line[0]['id_str'])\n","      text.append(line[0]['text'])\n","    df_data = pd.DataFrame({'id':source_id,'text':text, 'label':None})\n","  if read_label_filepath is not None:\n","    with open(read_label_filepath, 'r') as f:\n","      id, label = [], []\n","      temp = json.loads(f.read())\n","      for key, val in temp.items():\n","        id.append(key)\n","        label.append(val)\n","      # label_data = pd.DataFrame(list(zip(id, label)))\n","      # label_data.columns = ['id', 'label']\n","      for i in range(len(label)):\n","          label[i] = 1 if label[i] == \"rumour\" else 0 #convert_label(label_data[\"label\"][i])\n","          if source_id[i] == id[i]:\n","            df_data.label[i] = label[i]\n","    return df_data\n","  else:\n","    return df_data[['id', 'text']]\n","\n","df_dev = process_data('/content/drive/My Drive/data/dev.data.jsonl', '/content/drive/My Drive/data/dev.label.json')\n","df_train = process_data('/content/drive/My Drive/data/train.data.jsonl', '/content/drive/My Drive/data/train.label.json')\n","df_test = process_data('/content/drive/My Drive/data/test.data.jsonl', None)\n","\n","# a list of events, and each event is a list of tweets (source tweet + reactions)\n","print(\"Number of train data =\", len(df_train[\"text\"]))\n","print(\"Number of dev data =\", len(df_dev[\"text\"]))\n","print(\"Number of test data =\", len(df_test[\"text\"]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of train data = 4641\n","Number of dev data = 580\n","Number of test data = 581\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JnQ2N0Qo0x1-"},"source":["nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","# clean data\n","default_stopwords = set(nltk.corpus.stopwords.words('english'))\n","# will have to add the following custom\n","custom_stopwords = {\"http://\", \"rt\", \"co\", \"https://\", \"www\", \"@\"}\n","all_stopwords = default_stopwords | custom_stopwords\n","eng_stemmer = nltk.stem.SnowballStemmer('english')\n","tt = TweetTokenizer()\n","lemmatizer = WordNetLemmatizer()\n","\n","\n","def preprocess_data(df):\n","    print(\"Started preprocessing!\")\n","    text = df['text'].apply(str)\n","    for i in range(len(text)):\n","        text[i] = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+))', '', text[i])\n","        text[i] = re.sub(r'@[^\\s]+', '', text[i])\n","        text[i] = tt.tokenize(text[i])\n","        # remove single character words\n","        text[i] = [word for word in text[i] if len(word) > 1]\n","        # convert to lower case\n","        text[i] = [word.lower() for word in text[i]]\n","        # removing numbers\n","        text[i] = [word for word in text[i] if word.isalpha()]\n","        # stem the words\n","        text[i] = [lemmatizer.lemmatize(word) for word in text[i]]\n","        # remove stopwords\n","        text[i] = [word for word in text[i] if word not in default_stopwords]\n","        text[i] = \" \".join(text[i])\n","        df['text'][i] = text[i]\n","    return df['text']\n","\n","train_texts = preprocess_data(df_train)\n","dev_texts = preprocess_data(df_dev)\n","test_texts = preprocess_data(df_test)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"MThZURnkO9jU","executionInfo":{"status":"ok","timestamp":1619848367674,"user_tz":-600,"elapsed":6120,"user":{"displayName":"LIHUA WANG","photoUrl":"","userId":"14598865299805559806"}},"outputId":"d854b685-79b2-4ae5-87d8-01adb3a0a811"},"source":["test_texts[580]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'terrible news ottawa today thought prayer everyone involved'"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"zin9Rxv50xyH"},"source":["## Get Train data and Validation data for model 1\n","# x_train, x_test, y_train, y_test = train_test_split(train_texts, np.array(df_train[\"label\"], dtype=int), test_size=0.1)\n","x_train = train_texts\n","y_train = np.array(df_train[\"label\"].apply(int))\n","x_dev = dev_texts\n","y_dev = np.array(df_dev[\"label\"].apply(int))\n","x_test = test_texts"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IFajg78H0xwL"},"source":["def convert_label_tofile(label):\n","    if int(label) == 1:\n","        label = \"rumour\"\n","        return label\n","    else:\n","        label = \"non-rumour\"\n","        return label\n","\n","\n","def write_pred(id, labels, filename):\n","    for i in range(len(labels)):\n","        labels[i][0] = convert_label_tofile(labels[i][0])\n","    dic = collections.OrderedDict()\n","    for i in range(len(id)):\n","        dic[id[i]] = labels[i][0]\n","    with open(filename, 'w') as f:\n","        json.dump(dic, f)\n","    print(\"save finished\")\n","\n","\n","def convert_prob(y_pred):\n","    for i in range(len(y_pred)):\n","        if y_pred[i] > 0.5:\n","            y_pred[i] = 1\n","        else:\n","            y_pred[i] = 0\n","    return y_pred\n","\n","\n","def recall_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    return recall\n","\n","\n","def precision_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    return precision\n","\n","\n","def f1_m(y_true, y_pred):\n","    precision = precision_m(y_true, y_pred)\n","    recall = recall_m(y_true, y_pred)\n","    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fArvw_TS0xt2","executionInfo":{"status":"ok","timestamp":1619848374692,"user_tz":-600,"elapsed":13118,"user":{"displayName":"LIHUA WANG","photoUrl":"","userId":"14598865299805559806"}},"outputId":"444f2501-9602-4647-af1e-d48813cae394"},"source":["\"\"\"\n","BOW - NN\n","\"\"\"\n","tokenizer = Tokenizer(oov_token=\"<UNK>\")\n","tokenizer.fit_on_texts(x_train)\n","\n","bow_x_train = tokenizer.texts_to_matrix(x_train, mode=\"count\")  # BOW representation\n","bow_x_dev = tokenizer.texts_to_matrix(x_dev, mode=\"count\")  # BOW representation\n","bow_x_test = tokenizer.texts_to_matrix(x_test, mode=\"count\")  # BOW representation\n","vocab_size = bow_x_train.shape[1]\n","print(vocab_size)\n","\n","# model definition\n","# model construction\n","# embedding = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\"\n","# hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)\n","model1 = Sequential(name=\"feedforward-bow-input\")\n","# model1.add(hub_layer)\n","model1.add(layers.Dense(20, activation='relu'))\n","model1.add(layers.Dense(10, input_dim=vocab_size, activation='relu'))\n","model1.add(layers.Dense(1, activation='sigmoid'))\n","\n","# since it's a binary classification problem, we use a binary cross entropy loss here\n","model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc', f1_m, precision_m, recall_m])\n","# model1.summary()\n","\n","# training\n","model1.fit(bow_x_train, y_train, epochs=20, verbose=True, validation_data=(bow_x_dev, y_dev), batch_size=100)\n","loss, accuracy, f1_score, precision, recall = model1.evaluate(bow_x_dev, y_dev, verbose=False)\n","print(\"\\nTesting BOW - NN f1_score:  {:.4f}\".format(f1_score))\n","\n","# predict test set\n","y_pred_test1 = model1.predict(bow_x_test)\n","y_pred_test1 = convert_prob(y_pred_test1)\n","y_pred_test1 = np.array(y_pred_test1).tolist()\n","write_pred(df_test[\"id\"], y_pred_test1, \"test-output-bow.json\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["5086\n","Epoch 1/20\n","47/47 [==============================] - 1s 12ms/step - loss: 0.6858 - acc: 0.6070 - f1_m: 0.5412 - precision_m: 0.5470 - recall_m: 0.6608 - val_loss: 0.6220 - val_acc: 0.8586 - val_f1_m: 0.7599 - val_precision_m: 0.8479 - val_recall_m: 0.6960\n","Epoch 2/20\n","47/47 [==============================] - 0s 6ms/step - loss: 0.5746 - acc: 0.8707 - f1_m: 0.7913 - precision_m: 0.8430 - recall_m: 0.7503 - val_loss: 0.4524 - val_acc: 0.8690 - val_f1_m: 0.7859 - val_precision_m: 0.8197 - val_recall_m: 0.7609\n","Epoch 3/20\n","47/47 [==============================] - 0s 6ms/step - loss: 0.3850 - acc: 0.8918 - f1_m: 0.8340 - precision_m: 0.8553 - recall_m: 0.8172 - val_loss: 0.3456 - val_acc: 0.8741 - val_f1_m: 0.7908 - val_precision_m: 0.8488 - val_recall_m: 0.7479\n","Epoch 4/20\n","47/47 [==============================] - 0s 6ms/step - loss: 0.2614 - acc: 0.9175 - f1_m: 0.8787 - precision_m: 0.8920 - recall_m: 0.8686 - val_loss: 0.3083 - val_acc: 0.8793 - val_f1_m: 0.7996 - val_precision_m: 0.8552 - val_recall_m: 0.7560\n","Epoch 5/20\n","47/47 [==============================] - 0s 6ms/step - loss: 0.2013 - acc: 0.9326 - f1_m: 0.8955 - precision_m: 0.9092 - recall_m: 0.8853 - val_loss: 0.2960 - val_acc: 0.8845 - val_f1_m: 0.8082 - val_precision_m: 0.8684 - val_recall_m: 0.7643\n","Epoch 6/20\n","47/47 [==============================] - 0s 6ms/step - loss: 0.1571 - acc: 0.9469 - f1_m: 0.9194 - precision_m: 0.9298 - recall_m: 0.9104 - val_loss: 0.2924 - val_acc: 0.8914 - val_f1_m: 0.8255 - val_precision_m: 0.8593 - val_recall_m: 0.8037\n","Epoch 7/20\n","47/47 [==============================] - 0s 6ms/step - loss: 0.1308 - acc: 0.9575 - f1_m: 0.9385 - precision_m: 0.9409 - recall_m: 0.9376 - val_loss: 0.2987 - val_acc: 0.8759 - val_f1_m: 0.8043 - val_precision_m: 0.8162 - val_recall_m: 0.8017\n","Epoch 8/20\n","47/47 [==============================] - 0s 6ms/step - loss: 0.1108 - acc: 0.9659 - f1_m: 0.9507 - precision_m: 0.9620 - recall_m: 0.9405 - val_loss: 0.3064 - val_acc: 0.8776 - val_f1_m: 0.8056 - val_precision_m: 0.8251 - val_recall_m: 0.7976\n","Epoch 9/20\n","47/47 [==============================] - 0s 6ms/step - loss: 0.0884 - acc: 0.9719 - f1_m: 0.9544 - precision_m: 0.9669 - recall_m: 0.9445 - val_loss: 0.3177 - val_acc: 0.8793 - val_f1_m: 0.8099 - val_precision_m: 0.8182 - val_recall_m: 0.8141\n","Epoch 10/20\n","47/47 [==============================] - 0s 6ms/step - loss: 0.0733 - acc: 0.9801 - f1_m: 0.9712 - precision_m: 0.9776 - recall_m: 0.9661 - val_loss: 0.3293 - val_acc: 0.8793 - val_f1_m: 0.8097 - val_precision_m: 0.8277 - val_recall_m: 0.8059\n","Epoch 11/20\n","47/47 [==============================] - 0s 6ms/step - loss: 0.0643 - acc: 0.9812 - f1_m: 0.9726 - precision_m: 0.9780 - recall_m: 0.9681 - val_loss: 0.3470 - val_acc: 0.8793 - val_f1_m: 0.8061 - val_precision_m: 0.8422 - val_recall_m: 0.7853\n","Epoch 12/20\n","47/47 [==============================] - 0s 6ms/step - loss: 0.0568 - acc: 0.9816 - f1_m: 0.9729 - precision_m: 0.9820 - recall_m: 0.9644 - val_loss: 0.3614 - val_acc: 0.8707 - val_f1_m: 0.7966 - val_precision_m: 0.8170 - val_recall_m: 0.7920\n","Epoch 13/20\n","47/47 [==============================] - 0s 6ms/step - loss: 0.0522 - acc: 0.9821 - f1_m: 0.9713 - precision_m: 0.9752 - recall_m: 0.9685 - val_loss: 0.3819 - val_acc: 0.8690 - val_f1_m: 0.7959 - val_precision_m: 0.8097 - val_recall_m: 0.7962\n","Epoch 14/20\n","47/47 [==============================] - 0s 6ms/step - loss: 0.0415 - acc: 0.9874 - f1_m: 0.9814 - precision_m: 0.9797 - recall_m: 0.9834 - val_loss: 0.3940 - val_acc: 0.8707 - val_f1_m: 0.7978 - val_precision_m: 0.8135 - val_recall_m: 0.7962\n","Epoch 15/20\n","47/47 [==============================] - 0s 6ms/step - loss: 0.0377 - acc: 0.9902 - f1_m: 0.9846 - precision_m: 0.9865 - recall_m: 0.9831 - val_loss: 0.4127 - val_acc: 0.8672 - val_f1_m: 0.7918 - val_precision_m: 0.8126 - val_recall_m: 0.7873\n","Epoch 16/20\n","47/47 [==============================] - 0s 6ms/step - loss: 0.0315 - acc: 0.9917 - f1_m: 0.9870 - precision_m: 0.9853 - recall_m: 0.9893 - val_loss: 0.4340 - val_acc: 0.8655 - val_f1_m: 0.7894 - val_precision_m: 0.8081 - val_recall_m: 0.7871\n","Epoch 17/20\n","47/47 [==============================] - 0s 6ms/step - loss: 0.0290 - acc: 0.9944 - f1_m: 0.9917 - precision_m: 0.9952 - recall_m: 0.9884 - val_loss: 0.4516 - val_acc: 0.8672 - val_f1_m: 0.7922 - val_precision_m: 0.8123 - val_recall_m: 0.7873\n","Epoch 18/20\n","47/47 [==============================] - 0s 6ms/step - loss: 0.0274 - acc: 0.9915 - f1_m: 0.9878 - precision_m: 0.9910 - recall_m: 0.9850 - val_loss: 0.4720 - val_acc: 0.8655 - val_f1_m: 0.7897 - val_precision_m: 0.8080 - val_recall_m: 0.7873\n","Epoch 19/20\n","47/47 [==============================] - 0s 6ms/step - loss: 0.0238 - acc: 0.9941 - f1_m: 0.9921 - precision_m: 0.9947 - recall_m: 0.9897 - val_loss: 0.4906 - val_acc: 0.8621 - val_f1_m: 0.7806 - val_precision_m: 0.8126 - val_recall_m: 0.7645\n","Epoch 20/20\n","47/47 [==============================] - 0s 5ms/step - loss: 0.0195 - acc: 0.9954 - f1_m: 0.9934 - precision_m: 0.9947 - recall_m: 0.9922 - val_loss: 0.5090 - val_acc: 0.8569 - val_f1_m: 0.7743 - val_precision_m: 0.8005 - val_recall_m: 0.7645\n","\n","Testing BOW - NN f1_score:  0.7679\n","save finished\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ObvpqYR10xru","executionInfo":{"status":"ok","timestamp":1619848655885,"user_tz":-600,"elapsed":9057,"user":{"displayName":"LIHUA WANG","photoUrl":"","userId":"14598865299805559806"}},"outputId":"68353d97-d757-47d3-e545-a4baabed4489"},"source":["\"\"\"\n","word sequence\n","\"\"\"\n","xseq_train = tokenizer.texts_to_sequences(x_train)\n","xseq_dev = tokenizer.texts_to_sequences(x_dev)\n","xseq_test = tokenizer.texts_to_sequences(x_test)\n","#\n","maxlen = 100\n","xseq_train = pad_sequences(xseq_train, padding='post', maxlen=maxlen)\n","xseq_dev = pad_sequences(xseq_dev, padding='post', maxlen=maxlen)\n","xseq_test = pad_sequences(xseq_test, padding='post', maxlen=maxlen)\n","\n","embedding_dim = 10\n","\n","# word order preserved with this architecture\n","model2 = Sequential(name=\"feedforward-sequence-input\")\n","model2.add(layers.Embedding(input_dim=vocab_size,\n","                            output_dim=embedding_dim,\n","                            input_length=maxlen))\n","model2.add(layers.Flatten())\n","model2.add(layers.Dense(10, activation='relu'))\n","model2.add(layers.Dense(1, activation='sigmoid'))\n","model2.compile(optimizer='adam',\n","               loss='binary_crossentropy',\n","               metrics=['acc', f1_m, precision_m, recall_m])\n","model2.summary()\n","\n","# training\n","model2.fit(xseq_train, y_train, epochs=8, verbose=True, validation_data=(xseq_dev, y_dev), batch_size=10)\n","loss, accuracy, f1_score, precision, recall = model2.evaluate(xseq_dev, y_dev, verbose=False)\n","print(\"\\nTesting word sequence f1_score:  {:.4f}\".format(f1_score))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"feedforward-sequence-input\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_4 (Embedding)      (None, 100, 10)           50860     \n","_________________________________________________________________\n","flatten_4 (Flatten)          (None, 1000)              0         \n","_________________________________________________________________\n","dense_14 (Dense)             (None, 10)                10010     \n","_________________________________________________________________\n","dense_15 (Dense)             (None, 1)                 11        \n","=================================================================\n","Total params: 60,881\n","Trainable params: 60,881\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/8\n","465/465 [==============================] - 2s 2ms/step - loss: 0.6205 - acc: 0.6644 - f1_m: 0.0322 - precision_m: 0.0486 - recall_m: 0.0266 - val_loss: 0.4336 - val_acc: 0.8328 - val_f1_m: 0.6440 - val_precision_m: 0.8491 - val_recall_m: 0.5638\n","Epoch 2/8\n","465/465 [==============================] - 1s 2ms/step - loss: 0.4086 - acc: 0.8472 - f1_m: 0.7041 - precision_m: 0.8265 - recall_m: 0.6584 - val_loss: 0.3692 - val_acc: 0.8690 - val_f1_m: 0.7370 - val_precision_m: 0.8670 - val_recall_m: 0.6897\n","Epoch 3/8\n","465/465 [==============================] - 1s 2ms/step - loss: 0.2944 - acc: 0.9103 - f1_m: 0.8294 - precision_m: 0.8499 - recall_m: 0.8408 - val_loss: 0.2967 - val_acc: 0.8759 - val_f1_m: 0.7675 - val_precision_m: 0.8195 - val_recall_m: 0.7721\n","Epoch 4/8\n","465/465 [==============================] - 1s 2ms/step - loss: 0.1800 - acc: 0.9411 - f1_m: 0.8981 - precision_m: 0.8911 - recall_m: 0.9289 - val_loss: 0.3067 - val_acc: 0.8879 - val_f1_m: 0.7862 - val_precision_m: 0.8474 - val_recall_m: 0.7799\n","Epoch 5/8\n","465/465 [==============================] - 1s 2ms/step - loss: 0.1308 - acc: 0.9594 - f1_m: 0.9123 - precision_m: 0.9077 - recall_m: 0.9355 - val_loss: 0.3151 - val_acc: 0.8810 - val_f1_m: 0.7927 - val_precision_m: 0.8307 - val_recall_m: 0.8086\n","Epoch 6/8\n","465/465 [==============================] - 1s 2ms/step - loss: 0.1040 - acc: 0.9669 - f1_m: 0.9337 - precision_m: 0.9283 - recall_m: 0.9536 - val_loss: 0.3544 - val_acc: 0.8776 - val_f1_m: 0.7800 - val_precision_m: 0.8632 - val_recall_m: 0.7647\n","Epoch 7/8\n","465/465 [==============================] - 1s 2ms/step - loss: 0.0655 - acc: 0.9818 - f1_m: 0.9610 - precision_m: 0.9579 - recall_m: 0.9745 - val_loss: 0.3783 - val_acc: 0.8741 - val_f1_m: 0.7649 - val_precision_m: 0.8362 - val_recall_m: 0.7552\n","Epoch 8/8\n","465/465 [==============================] - 1s 2ms/step - loss: 0.0622 - acc: 0.9845 - f1_m: 0.9619 - precision_m: 0.9618 - recall_m: 0.9688 - val_loss: 0.3986 - val_acc: 0.8655 - val_f1_m: 0.7771 - val_precision_m: 0.8083 - val_recall_m: 0.8009\n","\n","Testing word sequence f1_score:  0.7522\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3yPU8MZ4fmA4","executionInfo":{"status":"ok","timestamp":1619848582336,"user_tz":-600,"elapsed":5861,"user":{"displayName":"LIHUA WANG","photoUrl":"","userId":"14598865299805559806"}},"outputId":"778324a9-7bfe-4cd1-8261-1a50e8c88330"},"source":["# predict test set\n","y_pred_test2 = model2.predict(xseq_test)\n","y_pred_test2 = convert_prob(y_pred_test2)\n","y_pred_test2 = np.array(y_pred_test2).tolist()\n","write_pred(df_test[\"id\"], y_pred_test2, \"test-output-seq.json\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["save finished\n"],"name":"stdout"}]}]}